{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross Entropy loss function\n",
    "\n",
    "used with softmax and cross entropy loss function\n",
    "\n",
    "![cross entropy loss function](assets\\softmax.png)\n",
    "![cross entropy loss function](assets\\softmax_layer.png)\n",
    "\n",
    "\n",
    "![cross entropy loss function](assets\\cross_entropy_loss_function.png)\n",
    "\n",
    "## in pytorch be careful\n",
    "\n",
    "![cross entropy loss function](assets\\cross_entropy_loss_function_care_ful_in_pytorch.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# y_predict \n",
    "# y = np.array([2,.1,.5],dtype = np.float32)\n",
    "def soft_max(x):\n",
    "    '''\n",
    "    params:\n",
    "        x (2d-array) : input with shape (n_sample,features)\n",
    "    '''\n",
    "    sum = np.sum(np.exp(x),axis=1) # (n_sample)\n",
    "    sum = np.reshape(sum,(sum.shape[0],1))\n",
    "    result = np.divide(np.exp(x) , sum)\n",
    "\n",
    "    return result\n",
    "\n",
    "def cross_entropy_loss(y,y_predict):\n",
    "\n",
    "    '''\n",
    "    params:\n",
    "        y (2d-array) : tesor with shape of (n_sample,)\n",
    "        y_predict (2d-array) : tensor with shape of (n_sample,features)\n",
    "    \n",
    "    returns:\n",
    "        loss (2d-array) : tensor with shape of (n_sample,1) \n",
    "        \n",
    "    '''\n",
    "    element_wise_multiply  = np.multiply(y,np.log(y_predict))\n",
    "    loss = -1/y.shape[0] * np.sum(element_wise_multiply,axis=1)\n",
    "    loss = loss.reshape(loss.shape[0],1)\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " y_predicted:[[2.44728471e-01 9.00305732e-02 6.65240956e-01]\n",
      " [1.66993610e-05 9.99859908e-01 1.23392515e-04]\n",
      " [9.98965779e-01 9.10938878e-04 1.23282171e-04]]\n",
      " loss :[[1.35868655e-01]\n",
      " [4.67005634e-05]\n",
      " [3.00034492e+00]]\n"
     ]
    }
   ],
   "source": [
    "multiple_sample = [\n",
    "    [2,1,3],\n",
    "    [3,14,5],\n",
    "    [10,3,1],\n",
    "]\n",
    "y_actual = [\n",
    "    [0,0,1],\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "]\n",
    "\n",
    "multiple_sample = np.array(multiple_sample)\n",
    "y_actual = np.array(y_actual)\n",
    "\n",
    "y_predict = soft_max(multiple_sample)\n",
    "\n",
    "loss = cross_entropy_loss(y=y_actual,y_predict=y_predict)\n",
    "\n",
    "print(f\" y_predicted:{y_predict}\")\n",
    "print(f\" loss :{loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  9, 11])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [\n",
    "    [1,2,4],\n",
    "    [2,3,4],\n",
    "    [1,4,6]\n",
    "]\n",
    "\n",
    "sample = np.array(sample)\n",
    "\n",
    "sum = np.sum(sample,axis=1)\n",
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_max = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_sample = [\n",
    "    [2,1,3],\n",
    "    [3,14,5],\n",
    "    [10,3,1],\n",
    "]\n",
    "multiple_sample = torch.tensor(multiple_sample,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " soft max output: tensor([[2.4473e-01, 9.0031e-02, 6.6524e-01],\n",
      "        [1.6699e-05, 9.9986e-01, 1.2339e-04],\n",
      "        [9.9897e-01, 9.1094e-04, 1.2328e-04]])\n"
     ]
    }
   ],
   "source": [
    "soft_max_out = soft_max(multiple_sample)\n",
    "print(f\" soft max output: {soft_max_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_sample = [\n",
    "    [2,1,3],\n",
    "    [3,14,5],\n",
    "    [10,3,1],\n",
    "]\n",
    "\n",
    "multiple_sample = torch.tensor(multiple_sample,dtype=torch.float32)\n",
    "\n",
    "y_good = [2,1,1]\n",
    "y_medium = [2,0,1]\n",
    "\n",
    "y_bad = [1,0,2]\n",
    "\n",
    "y_good = torch.tensor(y_good,dtype=torch.long)\n",
    "y_bad = torch.tensor(y_bad,dtype=torch.long)\n",
    "y_medium = torch.tensor(y_medium,dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_good :2.4695937633514404\n",
      "loss_bad :7.469593524932861\n",
      "loss_medium :6.136260509490967\n",
      "sample_class_prediction:torch.return_types.max(\n",
      "values=tensor([ 3., 14., 10.]),\n",
      "indices=tensor([2, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# will calcualte the loss of whole dataset\n",
    "\n",
    "loss_good = loss(multiple_sample,y_good)\n",
    "loss_bad = loss(multiple_sample,y_bad)\n",
    "loss_medium = loss(multiple_sample,y_medium)\n",
    "\n",
    "\n",
    "sample_class_predicition = torch.max(multiple_sample,dim=1)\n",
    "\n",
    "print(f\"loss_good :{loss_good}\")\n",
    "print(f\"loss_bad :{loss_bad}\")\n",
    "print(f\"loss_medium :{loss_medium}\")\n",
    "\n",
    "\n",
    "# now if the prediction is near the correc the loss will be less\n",
    "# if the prediction is not corret the loss will be high for whole batch or sample size\n",
    "print(f\"sample_class_prediction:{sample_class_predicition}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eg.\n",
    "![cross_entropy example](assets\\cross_entropy_example.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()  # (applies Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![binary_classification_problem](assets\\binary_classification_problem.png)\n",
    "![binary_classification_problem](assets\\binary_cross_entropy_loss_function.png)\n",
    "\n",
    "## labeling\n",
    "\n",
    "good prediction -> 1\n",
    "\n",
    "\n",
    "bad prediction -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "610c9d3786a5bcd122c38aa183fd980592c5db79f4cee84c2a544f8f000107e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
