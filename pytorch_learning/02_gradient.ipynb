{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How gradient is used\n",
    "\n",
    "Gradient is just the derivative value of output with respect to the weight which is used in backpropagation for weight adjustment\n",
    "\n",
    "![gradient](assets\\gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad = True ensures that the gradient are automatically calculated for each weights for each operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[0.1801, 0.6695, 0.7117, 0.5697],\n",
      "         [0.5046, 0.6651, 0.2095, 0.1856],\n",
      "         [0.3545, 0.7559, 0.8294, 0.2690]],\n",
      "\n",
      "        [[0.3658, 0.7116, 0.2215, 0.7136],\n",
      "         [0.2797, 0.4324, 0.8426, 0.9727],\n",
      "         [0.1415, 0.4659, 0.0742, 0.4353]]], requires_grad=True)\n",
      "a: tensor([[[2.1801, 2.6695, 2.7117, 2.5697],\n",
      "         [2.5046, 2.6651, 2.2095, 2.1856],\n",
      "         [2.3545, 2.7559, 2.8294, 2.2690]],\n",
      "\n",
      "        [[2.3658, 2.7116, 2.2215, 2.7136],\n",
      "         [2.2797, 2.4324, 2.8426, 2.9727],\n",
      "         [2.1415, 2.4659, 2.0742, 2.4353]]], grad_fn=<AddBackward0>)\n",
      "b: tensor([[[4.3603, 5.3391, 5.4235, 5.1395],\n",
      "         [5.0092, 5.3302, 4.4190, 4.3712],\n",
      "         [4.7090, 5.5117, 5.6589, 4.5379]],\n",
      "\n",
      "        [[4.7316, 5.4232, 4.4431, 5.4272],\n",
      "         [4.5594, 4.8648, 5.6852, 5.9454],\n",
      "         [4.2830, 4.9319, 4.1484, 4.8707]]], grad_fn=<MulBackward0>)\n",
      "c: tensor([[[19.0121, 28.5058, 29.4139, 26.4140],\n",
      "         [25.0916, 28.4110, 19.5280, 19.1071],\n",
      "         [22.1743, 30.3791, 32.0231, 20.5929]],\n",
      "\n",
      "        [[22.3881, 29.4115, 19.7410, 29.4546],\n",
      "         [20.7877, 23.6663, 32.3212, 35.3473],\n",
      "         [18.3443, 24.3233, 17.2093, 23.7234]]], grad_fn=<PowBackward0>)\n",
      "d: 24.89044952392578\n",
      "back_ward: None\n",
      "grad: tensor([[[0.7267, 0.8898, 0.9039, 0.8566],\n",
      "         [0.8349, 0.8884, 0.7365, 0.7285],\n",
      "         [0.7848, 0.9186, 0.9431, 0.7563]],\n",
      "\n",
      "        [[0.7886, 0.9039, 0.7405, 0.9045],\n",
      "         [0.7599, 0.8108, 0.9475, 0.9909],\n",
      "         [0.7138, 0.8220, 0.6914, 0.8118]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2,3,4),requires_grad=True)\n",
    "\n",
    "\n",
    "a = x + 2\n",
    "\n",
    "b = a*2\n",
    "\n",
    "c = b**2\n",
    "\n",
    "d = c.mean()\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")\n",
    "print(f\"d: {d}\")\n",
    "\n",
    "back_ward=d.backward() # d(d)/d(x)\n",
    "\n",
    "grad = x.grad\n",
    "\n",
    "print(f\"back_ward: {back_ward}\")\n",
    "print(f\"grad: {grad}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jacobian matrix with loss function\n",
    "\n",
    "![image](assets\\jacobian_matrix.png)\n",
    "\n",
    "jocobian matrix is calcuated from the computation graph and the loss function gradient is multiplied for the weight update with the help of appropriate optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[0.7233, 0.0123, 0.7041, 0.0693],\n",
      "         [0.8370, 0.8623, 0.5902, 0.3440],\n",
      "         [0.9012, 0.1902, 0.2526, 0.6914]],\n",
      "\n",
      "        [[0.3445, 0.5153, 0.0702, 0.5585],\n",
      "         [0.7818, 0.6864, 0.6772, 0.7779],\n",
      "         [0.9363, 0.5863, 0.7204, 0.3247]]], requires_grad=True)\n",
      "a: tensor([[[2.7233, 2.0123, 2.7041, 2.0693],\n",
      "         [2.8370, 2.8623, 2.5902, 2.3440],\n",
      "         [2.9012, 2.1902, 2.2526, 2.6914]],\n",
      "\n",
      "        [[2.3445, 2.5153, 2.0702, 2.5585],\n",
      "         [2.7818, 2.6864, 2.6772, 2.7779],\n",
      "         [2.9363, 2.5863, 2.7204, 2.3247]]], grad_fn=<AddBackward0>)\n",
      "b: tensor([[[5.4466, 4.0245, 5.4082, 4.1385],\n",
      "         [5.6741, 5.7246, 5.1804, 4.6880],\n",
      "         [5.8024, 4.3805, 4.5052, 5.3828]],\n",
      "\n",
      "        [[4.6891, 5.0306, 4.1404, 5.1170],\n",
      "         [5.5637, 5.3727, 5.3544, 5.5558],\n",
      "         [5.8727, 5.1725, 5.4407, 4.6494]]], grad_fn=<MulBackward0>)\n",
      "output: tensor([[[29.6658, 16.1968, 29.2491, 17.1275],\n",
      "         [32.1952, 32.7710, 26.8368, 21.9775],\n",
      "         [33.6680, 19.1884, 20.2971, 28.9745]],\n",
      "\n",
      "        [[21.9873, 25.3073, 17.1433, 26.1835],\n",
      "         [30.9546, 28.8661, 28.6694, 30.8670],\n",
      "         [34.4884, 26.7548, 29.6015, 21.6168]]], grad_fn=<PowBackward0>)\n",
      "grad: tensor([[[ 1.5222,  3.4252, 12.1256,  3.9701],\n",
      "         [20.8723,  8.1129, 20.5278, 15.5842],\n",
      "         [ 8.3271,  0.2042,  4.1929,  7.7410]],\n",
      "\n",
      "        [[ 6.8660,  7.2106,  1.1695, 13.0664],\n",
      "         [12.8424, 15.6290, 19.0763, 15.4335],\n",
      "         [18.3882,  8.9015, 20.7172, 17.0181]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand((2,3,4),requires_grad=True,dtype=torch.float32)\n",
    "\n",
    "\n",
    "a = input + 2\n",
    "\n",
    "b = a*2\n",
    "\n",
    "output= b**2\n",
    "\n",
    "\n",
    "print(f\"input: {input}\")\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"output: {output}\")\n",
    "\n",
    "# loss must me multiplied with the jocobian matrix to calculate the gradients\n",
    "loss = torch.rand((2,3,4),dtype=torch.float32)\n",
    "\n",
    "# calculating the gradient for backpropagation\n",
    "output.backward(loss) # d(output)/d(loss)*d(loss)/d(inputs)\n",
    "\n",
    "grad = input.grad\n",
    "\n",
    "print(f\"grad: {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing require gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________before__________________________\n",
      "tensor([[0.7146, 0.2951],\n",
      "        [0.7212, 0.2629]], requires_grad=True)\n",
      "___________________after__________________________\n",
      "tensor([[0.7146, 0.2951],\n",
      "        [0.7212, 0.2629]])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.rand((2,2),requires_grad=True)\n",
    "\n",
    "print(f\"___________________before__________________________\")\n",
    "print(weight)\n",
    "\n",
    "weight.requires_grad_(False)\n",
    "print(f\"___________________after__________________________\")\n",
    "print(weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________before__________________________\n",
      "tensor([[0.8682, 0.3246],\n",
      "        [0.8875, 0.9272]], requires_grad=True)\n",
      "___________________after__________________________\n",
      "tensor([[0.8682, 0.3246],\n",
      "        [0.8875, 0.9272]])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.rand((2,2),requires_grad=True)\n",
    "\n",
    "print(f\"___________________before__________________________\")\n",
    "print(weight)\n",
    "\n",
    "new_weight = weight.detach()\n",
    "print(f\"___________________after__________________________\")\n",
    "print(new_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________before__________________________\n",
      "tensor([[0.2268, 0.8826],\n",
      "        [0.1913, 0.2979]], requires_grad=True)\n",
      "___________________after__________________________\n",
      "tensor([[2.2268, 2.8826],\n",
      "        [2.1913, 2.2979]])\n",
      "tensor([[4.9586, 8.3093],\n",
      "        [4.8016, 5.2805]])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.rand((2,2),requires_grad=True)\n",
    "\n",
    "print(f\"___________________before__________________________\")\n",
    "print(weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"___________________after__________________________\")\n",
    "    a = weight + 2\n",
    "    b = a*a\n",
    "    \n",
    "    print(a)\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# avoiding accumulation of gradient in each forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: tensor([[0.5625, 0.6959, 0.0786],\n",
      "        [0.5783, 0.9495, 0.2622],\n",
      "        [0.0274, 0.6851, 0.8505]], requires_grad=True)\n",
      "------------------------iteration--0--------------------------\n",
      "output: 14.069750785827637\n",
      "grad: tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "------------------------iteration--1--------------------------\n",
      "output: 14.069750785827637\n",
      "grad: tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "------------------------iteration--2--------------------------\n",
      "output: 14.069750785827637\n",
      "grad: tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.rand(size=(3,3),requires_grad=True)\n",
    "print(f\"weights: {weights}\")\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"------------------------iteration--{i}--------------------------\")\n",
    "    output = (weights*3).sum()\n",
    "    \n",
    "\n",
    "    print(f\"output: {output}\")\n",
    "\n",
    "    error  = torch.tensor(1)\n",
    "    output.backward(error)\n",
    "\n",
    "\n",
    "    grad = weights.grad\n",
    "    print(f\"grad: {grad}\")\n",
    "    # must call grad.zero_() in each epoch to avoid accumulation \n",
    "    weights.grad.zero_()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.rand(3,requires_grad=True,dtype=torch.float)\n",
    "# optimizer = torch.optim.SGD(weights,lr=.01)\n",
    "\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "610c9d3786a5bcd122c38aa183fd980592c5db79f4cee84c2a544f8f000107e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
