{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # BERT Parameters\n",
    "maxlen = 30 # maximum of length\n",
    "batch_size = 6\n",
    "max_pred = 5  # max tokens of prediction\n",
    "n_layers = 6 # number of Encoder of Encoder Layer\n",
    "n_heads = 12 # number of heads in Multi-Head Attention\n",
    "d_model = 768 # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_batch(batch_size,sentences,token_list,word_dict,max_pred,maxlen,vocab_size,number_dict):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
    "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Proprecessing Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,maxlen,n_segments):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask,d_k):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,d_k,d_v,n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask,n_heads,d_v,d_model,d_k):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask,d_k=d_k)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,d_k,d_v,n_heads,d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model=d_model,d_k=d_k,d_v=d_v,n_heads=n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask,n_heads=n_heads,d_v=d_v,d_model=d_model,d_k=d_k) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self,d_model,n_layers,vocab_size,d_k,d_v,n_heads,d_ff):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size=vocab_size,d_model=d_model,maxlen=maxlen,n_segments=n_segments)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,d_k=d_k,d_v=d_v,n_heads=n_heads,d_ff=d_ff) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf\n",
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n'\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "    'Nice meet you too. How are you today?\\n'\n",
    "    'Great. My baseball team won the competition.\\n'\n",
    "    'Oh Congratulations, Juliet\\n'\n",
    "    'Thanks you Romeo'\n",
    ")\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)\n",
    "\n",
    "model = BERT(d_model=d_model,n_layers=n_layers,vocab_size=vocab_size,d_k = d_k,d_v=d_v,n_heads=n_heads,d_ff=d_ff)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch(batch_size=batch_size,sentences=sentences,token_list=token_list,word_dict=word_dict,max_pred=max_pred,maxlen=maxlen,vocab_size=vocab_size,number_dict=number_dict)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    pass\n",
    "    # optimizer.zero_grad()\n",
    "    # logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    # loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    # loss_lm = (loss_lm.float()).mean()\n",
    "    # loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "    # loss = loss_lm + loss_clsf\n",
    "    # if (epoch + 1) % 10 == 0:\n",
    "    #     print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    # loss.backward()\n",
    "    # optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about the preprocessing and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n'\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "    'Nice meet you too. How are you today?\\n'\n",
    "    'Great. My baseball team won the competition.\\n'\n",
    "    'Oh Congratulations, Juliet\\n'\n",
    "    'Thanks you Romeo'\n",
    ")\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n"
     ]
    }
   ],
   "source": [
    "# input is the text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you i am romeo', 'hello romeo my name is juliet nice to meet you', 'nice meet you too how are you today', 'great my baseball team won the competition', 'oh congratulations juliet', 'thanks you romeo']\n"
     ]
    }
   ],
   "source": [
    "# getting list of sentence from the text\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'my', 'too', 'juliet', 'you', 'name', 'competition', 'nice', 'i', 'great', 'how', 'meet', 'oh', 'team', 'won', 'baseball', 'romeo', 'today', 'are', 'is', 'hello', 'to', 'congratulations', 'thanks', 'the']\n"
     ]
    }
   ],
   "source": [
    "# splitting all sentence and getting the list of all unique words from text\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'am': 4, 'my': 5, 'too': 6, 'juliet': 7, 'you': 8, 'name': 9, 'competition': 10, 'nice': 11, 'i': 12, 'great': 13, 'how': 14, 'meet': 15, 'oh': 16, 'team': 17, 'won': 18, 'baseball': 19, 'romeo': 20, 'today': 21, 'are': 22, 'is': 23, 'hello': 24, 'to': 25, 'congratulations': 26, 'thanks': 27, 'the': 28}\n"
     ]
    }
   ],
   "source": [
    "# mapping the word to unique number(token) (which is called data transformation)\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: '[CLS]',\n",
       " 2: '[SEP]',\n",
       " 3: '[MASK]',\n",
       " 4: 'am',\n",
       " 5: 'my',\n",
       " 6: 'too',\n",
       " 7: 'juliet',\n",
       " 8: 'you',\n",
       " 9: 'name',\n",
       " 10: 'competition',\n",
       " 11: 'nice',\n",
       " 12: 'i',\n",
       " 13: 'great',\n",
       " 14: 'how',\n",
       " 15: 'meet',\n",
       " 16: 'oh',\n",
       " 17: 'team',\n",
       " 18: 'won',\n",
       " 19: 'baseball',\n",
       " 20: 'romeo',\n",
       " 21: 'today',\n",
       " 22: 'are',\n",
       " 23: 'is',\n",
       " 24: 'hello',\n",
       " 25: 'to',\n",
       " 26: 'congratulations',\n",
       " 27: 'thanks',\n",
       " 28: 'the'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse mapping of the unique number (token) to the word for prediction purpose\n",
    "number_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the dictionary \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hello how are you i am romeo',\n",
       "  'hello romeo my name is juliet nice to meet you',\n",
       "  'nice meet you too how are you today',\n",
       "  'great my baseball team won the competition',\n",
       "  'oh congratulations juliet',\n",
       "  'thanks you romeo'],\n",
       " [[24, 14, 22, 8, 12, 4, 20],\n",
       "  [24, 20, 5, 9, 23, 7, 11, 25, 15, 8],\n",
       "  [11, 15, 8, 6, 14, 22, 8, 21],\n",
       "  [13, 5, 19, 17, 18, 28, 10],\n",
       "  [16, 26, 7],\n",
       "  [27, 8, 20]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence and corresponding token_list\n",
    "sentences,token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upto now the \n",
    "```\n",
    "\n",
    "text -> sentence ( with removal of english [,./!?\"'etc])\n",
    "text -> extract each words -> word_list\n",
    "word_list ->(mapped to ) -> token_list \n",
    "vocab_size == len(world_list) \n",
    "\n",
    "each token_list contains the token_id of each word\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the sentences and converting to masked and segmented form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_batch(batch_size,sentences,token_list,word_dict,max_pred,maxlen,vocab_size,number_dict):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
    "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Proprecessing Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch(batch_size=batch_size,sentences=sentences,token_list=token_list,word_dict=word_dict,max_pred=max_pred,maxlen=maxlen,vocab_size=vocab_size,number_dict=number_dict)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up to Now\n",
    ">maxlen is maximum allowed words in the sentence\n",
    ">\n",
    ">n_sentence is the number of input sentence\n",
    ">\n",
    ">input_ids of each sentences : shape (n_sentence,maxlen)\n",
    ">\n",
    ">segment_ids of each sentence : shape (n_sentence,maxlen)\n",
    ">\n",
    ">masked_tokens of each sentence : shape (n_sentence,max_len)\n",
    ">\n",
    ">isNest of each sentence : shape (n_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 30]),\n",
       " tensor([[ 1, 11, 15,  8,  6,  3, 22,  8, 21,  2, 24,  3,  5,  9, 23,  7, 11, 25,\n",
       "          15,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 16, 26,  7,  2, 24, 20, 25,  9, 23,  7, 11, 25,  3,  8,  2,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 11, 15, 14,  6, 14, 22,  8, 21,  2, 16,  3,  7,  2,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 24,  3,  5,  9,  3,  7, 11, 25, 15,  8,  2, 11, 15,  8,  3, 14, 22,\n",
       "           8, 21,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 24,  3, 22,  8, 12,  4, 20,  2, 24,  3, 28,  9, 23,  7, 11, 25, 15,\n",
       "           8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 13,  5, 19, 17, 18, 28,  5,  2, 16, 26,  7,  2,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape,input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 30]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids.size(),segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 5]),\n",
       " tensor([[ 8, 14, 20,  0,  0],\n",
       "         [15,  5,  0,  0,  0],\n",
       "         [26,  8,  0,  0,  0],\n",
       "         [ 6, 23, 20,  0,  0],\n",
       "         [ 5, 20, 14,  0,  0],\n",
       "         [10, 17,  0,  0,  0]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens.shape,masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 5]),\n",
       " tensor([[19,  5, 11,  0,  0],\n",
       "         [13,  7,  0,  0,  0],\n",
       "         [11,  3,  0,  0,  0],\n",
       "         [15,  5,  2,  0,  0],\n",
       "         [11, 10,  2,  0,  0],\n",
       "         [ 7,  4,  0,  0,  0]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_pos.shape,masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]), tensor([0, 0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isNext.shape,isNext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,maxlen,n_segments):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 768, 30, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "norm = nn.LayerNorm(d_model)\n",
    "\n",
    "vocab_size,d_model,maxlen,n_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### up to now\n",
    "\n",
    "* vocab_size is the size of dictionary\n",
    "* d_model is the dimention of embedding to be performed\n",
    "* n_segments is number of segment of that the sentence is segmented for the training\n",
    "* max_len is the maximum length of each sentence\n",
    "\n",
    "> input to the nn.Embedding() \n",
    ">\n",
    ">A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    ">\n",
    "> nn.Embedding(unique_embdding_numbers,embeddng_dimension)\n",
    "\n",
    "* for token embedding -> Embedding(vocab_size,d_model) since there is need of embedding for all words\n",
    "* for position embedding -> Embedding(maxlen,d_model) since  there is only maxlen unique position\n",
    "* for segment embedding -> Embeddding(n_segment,d_model) since there is only two segments of each sentence\n",
    "\n",
    "d_model is common for embedding dimention because later on there are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 30]),\n",
       " torch.Size([6, 30, 768]),\n",
       " tensor([[[ 0.5609,  1.3569,  2.1036,  ..., -0.1036, -0.4089, -0.9100],\n",
       "          [ 0.9850,  2.4662,  0.2452,  ..., -1.6064, -0.4717,  2.6118],\n",
       "          [ 2.1469,  0.8265, -0.4476,  ...,  2.6386,  0.2635, -0.3848],\n",
       "          ...,\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770]],\n",
       " \n",
       "         [[ 0.5609,  1.3569,  2.1036,  ..., -0.1036, -0.4089, -0.9100],\n",
       "          [-0.3572,  1.0167, -0.6824,  ..., -0.0752, -0.9614,  0.2373],\n",
       "          [-1.7881, -2.3503,  1.4654,  ..., -0.0198, -0.2250, -0.3069],\n",
       "          ...,\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770]],\n",
       " \n",
       "         [[ 0.5609,  1.3569,  2.1036,  ..., -0.1036, -0.4089, -0.9100],\n",
       "          [ 0.9850,  2.4662,  0.2452,  ..., -1.6064, -0.4717,  2.6118],\n",
       "          [ 2.1469,  0.8265, -0.4476,  ...,  2.6386,  0.2635, -0.3848],\n",
       "          ...,\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770]],\n",
       " \n",
       "         [[ 0.5609,  1.3569,  2.1036,  ..., -0.1036, -0.4089, -0.9100],\n",
       "          [-1.6856,  1.0454,  2.1962,  ...,  0.9944,  0.9719, -0.7470],\n",
       "          [-1.5526,  0.7619,  1.1155,  ...,  0.7214, -0.1202, -1.0762],\n",
       "          ...,\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770]],\n",
       " \n",
       "         [[ 0.5609,  1.3569,  2.1036,  ..., -0.1036, -0.4089, -0.9100],\n",
       "          [-1.6856,  1.0454,  2.1962,  ...,  0.9944,  0.9719, -0.7470],\n",
       "          [-1.5526,  0.7619,  1.1155,  ...,  0.7214, -0.1202, -1.0762],\n",
       "          ...,\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770]],\n",
       " \n",
       "         [[ 0.5609,  1.3569,  2.1036,  ..., -0.1036, -0.4089, -0.9100],\n",
       "          [-0.8719,  1.6421, -0.3962,  ..., -0.9343, -1.8924,  2.9048],\n",
       "          [ 0.0482, -1.0493, -0.4533,  ..., -0.0047,  0.0738,  0.5210],\n",
       "          ...,\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770],\n",
       "          [-0.6187,  0.9441, -0.1023,  ..., -1.0172,  1.0107,  0.5770]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we supply input_ids of we get the corresponding embeddings\n",
    "input_embeddings = tok_embed(input_ids)\n",
    "input_ids.shape,input_embeddings.size() , input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq len : 30\n",
      "pos :tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]) \n",
      " pos shape : torch.Size([30])\n",
      "pos :tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]) \n",
      " pos shape : torch.Size([6, 30])\n"
     ]
    }
   ],
   "source": [
    "# when we input the position of each words in the sentence we will get the position embedding\n",
    "\n",
    "seq_len = input_ids.size(1)\n",
    "print(f\"seq len : {seq_len}\")\n",
    "pos = torch.arange(seq_len, dtype=torch.long)\n",
    "print(f\"pos :{pos} \\n pos shape : {pos.shape}\")\n",
    "pos = pos.unsqueeze(0).expand_as(input_ids)  # (seq_len,) -> (batch_size, seq_len)\n",
    "print(f\"pos :{pos} \\n pos shape : {pos.shape}\")\n",
    "pos_embedding = pos_embed(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 30]),\n",
       " torch.Size([6, 30]),\n",
       " torch.Size([6, 30, 768]),\n",
       " tensor([[[ 0.9660, -0.4986, -0.3459,  ...,  1.1357, -1.1111,  0.3926],\n",
       "          [-0.1513, -0.3111,  0.0345,  ..., -0.1109,  1.0082, -0.3238],\n",
       "          [-1.0357, -0.0436,  1.3298,  ..., -0.3325, -0.6350,  0.5875],\n",
       "          ...,\n",
       "          [-0.4983, -1.1975, -1.9269,  ..., -0.5401, -1.1938, -0.7832],\n",
       "          [ 0.6676, -0.1693,  0.5767,  ...,  1.2680,  1.2784, -0.9582],\n",
       "          [ 0.7070,  1.2319,  0.3383,  ..., -1.1873,  0.6569, -0.2067]],\n",
       " \n",
       "         [[ 0.9660, -0.4986, -0.3459,  ...,  1.1357, -1.1111,  0.3926],\n",
       "          [-0.1513, -0.3111,  0.0345,  ..., -0.1109,  1.0082, -0.3238],\n",
       "          [-1.0357, -0.0436,  1.3298,  ..., -0.3325, -0.6350,  0.5875],\n",
       "          ...,\n",
       "          [-0.4983, -1.1975, -1.9269,  ..., -0.5401, -1.1938, -0.7832],\n",
       "          [ 0.6676, -0.1693,  0.5767,  ...,  1.2680,  1.2784, -0.9582],\n",
       "          [ 0.7070,  1.2319,  0.3383,  ..., -1.1873,  0.6569, -0.2067]],\n",
       " \n",
       "         [[ 0.9660, -0.4986, -0.3459,  ...,  1.1357, -1.1111,  0.3926],\n",
       "          [-0.1513, -0.3111,  0.0345,  ..., -0.1109,  1.0082, -0.3238],\n",
       "          [-1.0357, -0.0436,  1.3298,  ..., -0.3325, -0.6350,  0.5875],\n",
       "          ...,\n",
       "          [-0.4983, -1.1975, -1.9269,  ..., -0.5401, -1.1938, -0.7832],\n",
       "          [ 0.6676, -0.1693,  0.5767,  ...,  1.2680,  1.2784, -0.9582],\n",
       "          [ 0.7070,  1.2319,  0.3383,  ..., -1.1873,  0.6569, -0.2067]],\n",
       " \n",
       "         [[ 0.9660, -0.4986, -0.3459,  ...,  1.1357, -1.1111,  0.3926],\n",
       "          [-0.1513, -0.3111,  0.0345,  ..., -0.1109,  1.0082, -0.3238],\n",
       "          [-1.0357, -0.0436,  1.3298,  ..., -0.3325, -0.6350,  0.5875],\n",
       "          ...,\n",
       "          [-0.4983, -1.1975, -1.9269,  ..., -0.5401, -1.1938, -0.7832],\n",
       "          [ 0.6676, -0.1693,  0.5767,  ...,  1.2680,  1.2784, -0.9582],\n",
       "          [ 0.7070,  1.2319,  0.3383,  ..., -1.1873,  0.6569, -0.2067]],\n",
       " \n",
       "         [[ 0.9660, -0.4986, -0.3459,  ...,  1.1357, -1.1111,  0.3926],\n",
       "          [-0.1513, -0.3111,  0.0345,  ..., -0.1109,  1.0082, -0.3238],\n",
       "          [-1.0357, -0.0436,  1.3298,  ..., -0.3325, -0.6350,  0.5875],\n",
       "          ...,\n",
       "          [-0.4983, -1.1975, -1.9269,  ..., -0.5401, -1.1938, -0.7832],\n",
       "          [ 0.6676, -0.1693,  0.5767,  ...,  1.2680,  1.2784, -0.9582],\n",
       "          [ 0.7070,  1.2319,  0.3383,  ..., -1.1873,  0.6569, -0.2067]],\n",
       " \n",
       "         [[ 0.9660, -0.4986, -0.3459,  ...,  1.1357, -1.1111,  0.3926],\n",
       "          [-0.1513, -0.3111,  0.0345,  ..., -0.1109,  1.0082, -0.3238],\n",
       "          [-1.0357, -0.0436,  1.3298,  ..., -0.3325, -0.6350,  0.5875],\n",
       "          ...,\n",
       "          [-0.4983, -1.1975, -1.9269,  ..., -0.5401, -1.1938, -0.7832],\n",
       "          [ 0.6676, -0.1693,  0.5767,  ...,  1.2680,  1.2784, -0.9582],\n",
       "          [ 0.7070,  1.2319,  0.3383,  ..., -1.1873,  0.6569, -0.2067]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_ids.shape,pos.shape,pos_embedding.shape,pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 30]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we give segment_ids we will get the segment embedding\n",
    "segment_ids.shape,segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 30]),\n",
       " torch.Size([6, 30, 768]),\n",
       " tensor([[[-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          ...,\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930]],\n",
       " \n",
       "         [[-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          ...,\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930]],\n",
       " \n",
       "         [[-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          ...,\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930]],\n",
       " \n",
       "         [[-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          ...,\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930]],\n",
       " \n",
       "         [[-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          ...,\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930]],\n",
       " \n",
       "         [[-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          ...,\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930],\n",
       "          [-3.1898, -0.6039,  0.8623,  ..., -0.0888,  0.3330,  2.1930]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seg_embedding = seg_embed(segment_ids)\n",
    "segment_ids.shape,seg_embedding.shape,seg_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is query? what is key? what is value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first getting the attention padding mask\n",
    "\n",
    "padding mask helps to distinquish between which is word and which is not word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k): # seq_q = input_ids , seq_k = input_ids\n",
    "    batch_size, len_q = seq_q.size() # taking len_q as size of max_len\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 30, 30]),\n",
       " tensor([[[False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "enc_self_attn_mask.shape,enc_self_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size : 6\n"
     ]
    }
   ],
   "source": [
    "seq_q,seq_k = input_ids,input_ids\n",
    "batch_size, len_q = seq_q.size()\n",
    "print(f\"batch size : {batch_size}\")\n",
    "batch_size, len_k = seq_k.size()\n",
    "# eq(zero) is PAD token\n",
    "pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "enc_self_attn_mask =  pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    ">>> x = torch.tensor([[1], [2], [3]])\n",
    ">>> x.size()\n",
    "torch.Size([3, 1])\n",
    ">>> x.expand(3, 4)\n",
    "tensor([[ 1,  1,  1,  1],\n",
    "        [ 2,  2,  2,  2],\n",
    "        [ 3,  3,  3,  3]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding: tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "torch.Size([6, 30])\n",
      "torch.Size([6, 1, 30])\n",
      "torch.Size([6, 30, 30])\n",
      "expanded shape: tensor([[[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         ...,\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         ...,\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         ...,\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         ...,\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         ...,\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         ...,\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "x = seq_k.data.eq(0)\n",
    "print(f'padding: {x}')\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(1)\n",
    "print(x.shape)\n",
    "x = x.expand(batch_size,len_q,len_k)\n",
    "print(x.shape)\n",
    "print(f\"expanded shape: {x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(x[0,0]==x[0,1])\n",
    "print(x[0,1,0] == x[0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upto now\n",
    "\n",
    "shape of padding mask : (n_batch,n_q,n_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output of 12 encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape:torch.Size([6, 30, 768])\n",
      "torch.Size([6, 30, 768]) torch.Size([6, 12, 30, 30])\n",
      "ouput of encoder : torch.Size([6, 30, 768]) \n",
      "ouput of enc_self_attn : torch.Size([6, 12, 30, 30]) \n"
     ]
    }
   ],
   "source": [
    "# embdding layer\n",
    "embedding = Embedding(vocab_size=vocab_size,d_model=d_model,maxlen=maxlen,n_segments=n_segments)\n",
    "\n",
    "# embedding output\n",
    "output = embedding(input_ids, segment_ids)\n",
    "print(f'embedding shape:{output.shape}')\n",
    "\n",
    "# padding of the key, query , value\n",
    "enc_self_attn_mask = get_attn_pad_mask(seq_q=input_ids,seq_k=input_ids)\n",
    "\n",
    "# 6 encoder layers\n",
    "layers = nn.ModuleList([EncoderLayer(d_model=d_model,d_k=d_k,n_heads=n_heads,d_v=d_v,d_ff=d_ff) for _ in range(n_layers)])\n",
    "\n",
    "# ouput of encoder layers\n",
    "# inputs are output : embedding ouput , enc_self_attn_mask = padding mask of sentences\n",
    "output_list = []\n",
    "for layer in layers:\n",
    "    output,enc_self_attn = layer(output,enc_self_attn_mask)\n",
    "    output_list.append([output,enc_self_attn])\n",
    "\n",
    "print(output.shape,enc_self_attn.shape)\n",
    "print(f\"ouput of encoder : {output.shape} \")\n",
    "print(f\"ouput of enc_self_attn : {enc_self_attn.shape} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,d_k,d_v,n_heads,d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model=d_model,d_k=d_k,d_v=d_v,n_heads=n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask,n_heads=n_heads,d_v=d_v,d_model=d_model,d_k=d_k) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "encorder_layer = EncoderLayer(d_model=d_model,d_k=d_k,d_v=d_v,n_heads=n_heads,d_ff=d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding output shape:torch.Size([6, 30, 768])\n",
      "ouput of encoder : torch.Size([6, 30, 768]) \n",
      "ouput of enc_self_attn : torch.Size([6, 12, 30, 30]) \n"
     ]
    }
   ],
   "source": [
    "# for single encoder\n",
    "\n",
    "# embedding layer\n",
    "embedding = Embedding(vocab_size=vocab_size,d_model=d_model,maxlen=maxlen,n_segments=n_segments)\n",
    "\n",
    "# output of embedding\n",
    "embedding_output = embedding(input_ids, segment_ids)\n",
    "print(f'embedding output shape:{embedding_output.shape}')\n",
    "\n",
    "# padding mask for the self attentiion \n",
    "enc_self_attn_mask = get_attn_pad_mask(seq_q=input_ids,seq_k=input_ids)\n",
    "\n",
    "# whole 12 encoder layers\n",
    "layers = nn.ModuleList([EncoderLayer(d_model=d_model,d_k=d_k,n_heads=n_heads,d_v=d_v,d_ff=d_ff) for _ in range(n_layers)])\n",
    "\n",
    "\n",
    "output, enc_self_attn = encorder_layer(embedding_output,enc_self_attn_mask)\n",
    "\n",
    "print(f\"ouput of encoder : {output.shape} \")\n",
    "print(f\"ouput of enc_self_attn : {enc_self_attn.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding output shape: torch.Size([6, 30, 768])\n",
      "ouput of encoder output : torch.Size([6, 30, 768]) \n",
      "ouput of encoder self attention mask : torch.Size([6, 30, 30]) \n",
      "ouput of encoder output after position wise feed forward : torch.Size([6, 30, 768]) \n",
      "length of query, key, value and maxlen are same:True\n"
     ]
    }
   ],
   "source": [
    "# multi head attention layer\n",
    "enc_self_attn = MultiHeadAttention(d_model=d_model,d_k=d_k,d_v=d_v,n_heads=n_heads)\n",
    "\n",
    "# position wise feed forward network\n",
    "pos_ffn = PoswiseFeedForwardNet(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "# embedding layer \n",
    "embedding_layer =  Embedding(vocab_size=vocab_size,d_model=d_model,maxlen=maxlen,n_segments=n_segments)\n",
    "\n",
    "# embedding ouput\n",
    "embedding_output = embedding_layer(input_ids,segment_ids)\n",
    "print(f'embedding output shape: {embedding_output.shape}')\n",
    "\n",
    "# input to multi_head attentions\n",
    "# 1. enc_inputs = embedding output shape(n_batch*maxlen*d_model)\n",
    "# 2. enc_self_attn_mask = padding mask of query , key with shape [n_batch * max_len , max_len]\n",
    "enc_inputs = embedding_output\n",
    "enc_self_attn_mask = get_attn_pad_mask(seq_q = input_ids,seq_k=input_ids)\n",
    "enc_outputs, attn = enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask,n_heads=n_heads,d_v=d_v,d_model=d_model,d_k=d_k) # enc_inputs to same Q,K,V\n",
    "\n",
    "print(f\"ouput of encoder output : {enc_outputs.shape} \")\n",
    "\n",
    "\n",
    "enc_outputs = pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "\n",
    "\n",
    "print(f\"ouput of encoder self attention mask : {enc_self_attn_mask.shape} \")\n",
    "print(f\"ouput of encoder output after position wise feed forward : {enc_outputs.shape} \")\n",
    "print(f\"length of query, key, value and maxlen are same:{len_q == len_k == maxlen}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding output shape: torch.Size([6, 30, 768])\n",
      "ouput of encoder output : torch.Size([6, 30, 768]) \n",
      "ouput of encoder self attention mask : torch.Size([6, 30, 30]) \n",
      "ouput of encoder output after position wise feed forward : torch.Size([6, 30, 768]) \n",
      "length of query, key, value and maxlen are same:True\n"
     ]
    }
   ],
   "source": [
    "# multi head attention layer\n",
    "enc_self_attn = MultiHeadAttention(d_model=d_model,d_k=d_k,d_v=d_v,n_heads=n_heads)\n",
    "\n",
    "# position wise feed forward network\n",
    "pos_ffn = PoswiseFeedForwardNet(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "# embedding layer \n",
    "embedding_layer =  Embedding(vocab_size=vocab_size,d_model=d_model,maxlen=maxlen,n_segments=n_segments)\n",
    "\n",
    "# embedding ouput\n",
    "embedding_output = embedding_layer(input_ids,segment_ids)\n",
    "print(f'embedding output shape: {embedding_output.shape}')\n",
    "\n",
    "# input to multi_head attentions\n",
    "# 1. enc_inputs = embedding output shape(n_batch*maxlen*d_model)\n",
    "# 2. enc_self_attn_mask = padding mask of query , key with shape [n_batch * max_len , max_len]\n",
    "enc_inputs = embedding_output\n",
    "enc_self_attn_mask = get_attn_pad_mask(seq_q = input_ids,seq_k=input_ids)\n",
    "enc_outputs, attn = enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask,n_heads=n_heads,d_v=d_v,d_model=d_model,d_k=d_k) # enc_inputs to same Q,K,V\n",
    "\n",
    "print(f\"ouput of encoder output : {enc_outputs.shape} \")\n",
    "\n",
    "\n",
    "enc_outputs = pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "\n",
    "\n",
    "print(f\"ouput of encoder self attention mask : {enc_self_attn_mask.shape} \")\n",
    "print(f\"ouput of encoder output after position wise feed forward : {enc_outputs.shape} \")\n",
    "print(f\"length of query, key, value and maxlen are same:{len_q == len_k == maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,d_k,d_v,n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask,n_heads,d_v,d_model,d_k):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask,d_k=d_k)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "d_k : 64\n"
     ]
    }
   ],
   "source": [
    "W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "print(W_Q)\n",
    "print(f\"d_k : {d_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,K,V = enc_inputs , enc_inputs, enc_inputs\n",
    "d_q = d_k = d_v  # 64 dimention of query , key , value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of q_s: torch.Size([6, 30, 768])\n",
      "shape of q_s: torch.Size([6, 30, 12, 64])\n",
      "shape of q_s: torch.Size([6, 12, 30, 64])\n",
      "shape of v_s: torch.Size([6, 12, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# residual is simply the embedding ouput or encoding input\n",
    "\n",
    "# batch_size is simply the n_batch\n",
    "residual, batch_size = Q, Q.size(0)\n",
    "\n",
    "# \n",
    "q_s = W_Q(Q)\n",
    "print(f\"shape of q_s: {q_s.shape}\")\n",
    "\n",
    "q_s = q_s.view(batch_size,-1,n_heads,d_k)\n",
    "print(f\"shape of q_s: {q_s.shape}\")\n",
    "\n",
    "q_s = q_s.transpose(1,2) # initially dim : 0 -> batch , 1 -> maxlen , 2 -> n_heads , 3 -> d_k\n",
    "# here 1 and 2 are enterchanged for making such that each head contains the all the words with d_k number of embedding\n",
    "\n",
    "# after transposing dim:  0 -> batch , 1 -> n_heads , 2 -> len_q, maxlen , 3 -> d_k\n",
    "\n",
    "print(f\"shape of q_s: {q_s.shape}\")\n",
    "\n",
    "v_s = W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "print(f\"shape of v_s: {v_s.shape}\")\n",
    "\n",
    "\n",
    "k_s = W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "v_s = W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*purpose of linear layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 4, 768]), torch.Size([1, 2, 3, 4, 2]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand((1,2,3,4,768))\n",
    "\n",
    "# linear only cares about the final axis element\n",
    "# and takes each element as input for input layer of linear\n",
    "# and ouput is the same shape execpt the last axis has increased or decreased length of axis \n",
    "linear_layer =  nn.Linear(in_features=768,out_features=2)\n",
    "linear_out = linear_layer(input)\n",
    "\n",
    "input.shape , linear_out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of attention mask : torch.Size([6, 30, 30])\n",
      "shape of attn_mask : torch.Size([6, 1, 30, 30])\n",
      "shape of attn_mask : torch.Size([6, 12, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = get_attn_pad_mask(seq_q=input_ids,seq_k = input_ids)\n",
    "\n",
    "print(f\"shape of attention mask : {attn_mask.shape}\") # shape(n_bath,max_len,max_len) # may be for query and value length\n",
    "\n",
    "# adding extra dimention for the single head\n",
    "attn_mask = attn_mask.unsqueeze(dim = 1) # initially : shape(n_batch,max_len,max_len) \n",
    "# aftter : shape(n_batch,1-head,maxlen,maxlen)\n",
    "\n",
    "print(f\"shape of attn_mask : {attn_mask.shape}\")\n",
    "\n",
    "# INFO: Parameters\n",
    "# sizes (torch.Size or int...) – The number of times to repeat this tensor along each dimension\n",
    "\n",
    "# after that repeating the same row for making the n_heads or 12 heads\n",
    "attn_mask = attn_mask.repeat(1, n_heads, 1, 1) \n",
    "# here repeat(\n",
    "# 1 -> for single repeat or no change\n",
    "# n_heads -> for n_heads times\n",
    "# )\n",
    "\n",
    "\n",
    "print(f\"shape of attn_mask : {attn_mask.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaled dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask,d_k):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(d_k,dtype=torch.float32)) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tensor: torch.Size([1, 2, 3, 4])\n",
      "after transpose shape of tensor: torch.Size([1, 2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "test_1 = torch.rand((1,2,3,4))\n",
    "\n",
    "print(f\"shape of tensor: {test_1.shape}\")\n",
    "\n",
    "test_2 = test_1.transpose(dim0=-2,dim1=-1)\n",
    "\n",
    "print(f\"after transpose shape of tensor: {test_2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Q , K , V : (torch.Size([6, 12, 30, 64]), torch.Size([6, 12, 30, 64]), torch.Size([6, 12, 30, 64]))\n",
      "shape of scores:torch.Size([6, 12, 30, 30])\n",
      "shape of attn_mask : torch.Size([6, 12, 30, 30])\n",
      "before masking: tensor([-0.2807,  0.0675,  0.5605, -0.5441, -0.1867,  0.0929, -0.1006, -0.0157,\n",
      "        -0.5089, -0.4573, -0.2695, -0.2615, -0.3499,  0.2060, -0.0142,  0.1995,\n",
      "         0.0128, -0.2257,  0.2191, -0.1056, -0.3517, -0.0495, -0.0107,  0.2420,\n",
      "        -0.1038,  0.1172,  0.0740, -0.0520,  0.1095,  0.4185],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "after masking : tensor([-2.8068e-01,  6.7542e-02,  5.6048e-01, -5.4405e-01, -1.8671e-01,\n",
      "         9.2885e-02, -1.0061e-01, -1.5747e-02, -5.0886e-01, -4.5725e-01,\n",
      "        -2.6954e-01, -2.6154e-01, -3.4991e-01,  2.0605e-01, -1.4227e-02,\n",
      "         1.9947e-01,  1.2781e-02, -2.2565e-01,  2.1912e-01, -1.0557e-01,\n",
      "        -3.5168e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "shape of context : torch.Size([6, 12, 30, 64]) \n"
     ]
    }
   ],
   "source": [
    "Q = q_s # shape(n_batch,n_heads,maxlen,d_q)\n",
    "K = k_s \n",
    "V = v_s\n",
    "# batch matrix multiplication is performed since (dimension of at least 1 tensor is greater than 2 see rule for that)\n",
    "# https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "\n",
    "print(f\"shape of Q , K , V : {Q.shape,K.shape,V.shape}\")\n",
    "\n",
    "# here matrix multiplication is between the (maxlen and d_q) and (d_k and maxlen) dimention rest are unaffected\n",
    "# dim: maxlen -> number of words in sentenc \n",
    "# dim: d_k -> length of reduced key dimention from the input_ids embedding  \n",
    "scores = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt( torch.tensor(d_k,dtype=torch.float32)) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "print(f\"shape of scores:{scores.shape}\")\n",
    "print(f\"shape of attn_mask : {attn_mask.shape}\")\n",
    "print(f\"before masking: {scores[0,0,0]}\")\n",
    "scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "print(f\"after masking : {scores[0,0,0]}\")\n",
    "\n",
    "# applying soft mask in last dimention\n",
    "attn = nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "# finally context of each word is calculated with matrix multiplication \n",
    "# for finding out the which word are more important in that context\n",
    "# first matrix multiplication helps to understand the which word is important and affect the result most as also the called attention filter\n",
    "# and send matrix multiplication helps to apply that attention filter just like convolutional neural network to the actual sentence or (embedding inputs)\n",
    "context = torch.matmul(attn, V)\n",
    "print(f\"shape of context : {context.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 12, 30, 64]), torch.Size([6, 12, 30, 30]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_s.shape,attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "completed the scaled dot product\n",
    "\n",
    "### remaining multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of attention mask : torch.Size([6, 30, 30])\n",
      "shape of attn_mask : torch.Size([6, 1, 30, 30])\n",
      "shape of attn_mask : torch.Size([6, 12, 30, 30])\n",
      "shape of context, attention: (torch.Size([6, 12, 30, 64]), torch.Size([6, 12, 30, 30]))\n",
      "shape of context after transpose: torch.Size([6, 30, 12, 64])\n",
      "shape of context after contiguous: torch.Size([6, 30, 12, 64])\n",
      "shape of context: torch.Size([6, 30, 768])\n",
      "linear output shape : torch.Size([6, 30, 768])\n",
      "normalized  shape : torch.Size([6, 30, 768])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = get_attn_pad_mask(seq_q=input_ids,seq_k = input_ids)\n",
    "\n",
    "print(f\"shape of attention mask : {attn_mask.shape}\") # shape(n_bath,max_len,max_len) # may be for query and value length\n",
    "\n",
    "# adding extra dimention for the single head\n",
    "attn_mask = attn_mask.unsqueeze(dim = 1) # initially : shape(n_batch,max_len,max_len) \n",
    "# aftter : shape(n_batch,1-head,maxlen,maxlen)\n",
    "\n",
    "print(f\"shape of attn_mask : {attn_mask.shape}\")\n",
    "\n",
    "# INFO: Parameters\n",
    "# sizes (torch.Size or int...) – The number of times to repeat this tensor along each dimension\n",
    "\n",
    "# after that repeating the same row for making the n_heads or 12 heads\n",
    "attn_mask = attn_mask.repeat(1, n_heads, 1, 1) \n",
    "# here repeat(\n",
    "# 1 -> for single repeat or no change\n",
    "# n_heads -> for n_heads times\n",
    "# )\n",
    "\n",
    "\n",
    "print(f\"shape of attn_mask : {attn_mask.shape}\")\n",
    "\n",
    "\n",
    "# context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask,d_k=d_k)\n",
    "\n",
    "# context shape(n_batch,n_head,maxlen,d_k) , attention shape(n_batch,n_head,n_q,n_k)\n",
    "print(f\"shape of context, attention: {context.shape,attn.shape}\")\n",
    "\n",
    "context = context.transpose(1,2) # n_head and n_q transpose to make shape(n_batch,n_q,n_head,d_v)\n",
    "print(f\"shape of context after transpose: {context.shape}\")\n",
    "\n",
    "\n",
    "context = context.contiguous() \n",
    "print(f\"shape of context after contiguous: {context.shape}\")\n",
    "\n",
    "# finally we return the context that gives the whole view sentence \n",
    "context = context.view(batch_size,-1,n_heads*d_v)\n",
    "\n",
    "# context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "\n",
    "print(f\"shape of context: {context.shape}\")\n",
    "\n",
    "output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "\n",
    "print(f\"linear output shape : {output.shape}\")\n",
    "\n",
    "# layer normalization across the axis of feature it's sentence or maxlen or len_q dimention\n",
    "output = nn.LayerNorm(d_model)(output + residual) # output: [batch_size x len_q x d_model]\n",
    "\n",
    "print(f\"normalized  shape : {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model == n_heads * d_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------\n",
    "multi head attention completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 768)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ff,d_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final output shape: torch.Size([6, 30, 768])\n"
     ]
    }
   ],
   "source": [
    "poswise_feed_forward = PoswiseFeedForwardNet(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "output = poswise_feed_forward(output)\n",
    "\n",
    "print(f\"final output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra layers of bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape:torch.Size([6, 30, 768])\n",
      "torch.Size([6, 30, 768]) torch.Size([6, 12, 30, 30])\n",
      "ouput of encoder : torch.Size([6, 30, 768]) \n",
      "ouput of enc_self_attn : torch.Size([6, 12, 30, 30]) \n",
      "embedding weight shape : torch.Size([29, 768])\n",
      "n_vocab ,n_dim : (29, 768)\n",
      "decoder weight and embedding weight share same memory: True\n",
      "shape of output: torch.Size([6, 30, 768])\n",
      "shape of output[:,0] : torch.Size([6, 768])\n",
      " shape of h_pooled : torch.Size([6, 768])\n",
      " shape of logits_clsf : torch.Size([6, 2])\n",
      "logits_clsf : tensor([[ 0.0304, -0.0858],\n",
      "        [ 0.0342, -0.0787],\n",
      "        [ 0.0343, -0.0887],\n",
      "        [ 0.0342, -0.0859],\n",
      "        [ 0.0475, -0.0715],\n",
      "        [ 0.0333, -0.0792]], grad_fn=<AddmmBackward0>)\n",
      " shape of masked_pos: torch.Size([6, 5, 768])\n",
      "shape of final output: torch.Size([6, 30, 768])\n",
      "output : tensor([[[ 0.1532, -0.1147,  0.1736,  ...,  0.0703, -0.0467,  0.0740],\n",
      "         [ 0.0466, -0.0760,  0.0523,  ...,  0.2266,  0.0851,  0.1634],\n",
      "         [ 0.1544, -0.1170,  0.0933,  ...,  0.2966, -0.1303,  0.2422],\n",
      "         ...,\n",
      "         [ 0.2411,  0.2154,  0.0230,  ...,  0.3016, -0.0066,  0.2114],\n",
      "         [ 0.1771,  0.0872,  0.0663,  ...,  0.2309, -0.1366,  0.1442],\n",
      "         [ 0.1491, -0.0152,  0.0946,  ...,  0.2371, -0.0278,  0.0458]],\n",
      "\n",
      "        [[ 0.1928, -0.1364,  0.1355,  ...,  0.0802, -0.0259,  0.1209],\n",
      "         [-0.0072, -0.0160,  0.0352,  ...,  0.0343, -0.1525,  0.0686],\n",
      "         [ 0.1718,  0.0587,  0.0126,  ...,  0.3012, -0.0247,  0.3248],\n",
      "         ...,\n",
      "         [ 0.2732,  0.2475, -0.0163,  ...,  0.3995,  0.0058,  0.2505],\n",
      "         [ 0.1622,  0.1338, -0.0138,  ...,  0.2528, -0.1040,  0.1391],\n",
      "         [ 0.1834,  0.0037,  0.0860,  ...,  0.2767, -0.0301,  0.0354]],\n",
      "\n",
      "        [[ 0.1910, -0.1339,  0.1811,  ...,  0.0929, -0.0430,  0.0806],\n",
      "         [ 0.0055, -0.0663,  0.0757,  ...,  0.0613, -0.1308,  0.0603],\n",
      "         [ 0.2221,  0.0746,  0.1906,  ...,  0.1172, -0.0420,  0.3600],\n",
      "         ...,\n",
      "         [ 0.2466,  0.1922,  0.0224,  ...,  0.2864,  0.0057,  0.2143],\n",
      "         [ 0.1894,  0.0709,  0.0902,  ...,  0.1827, -0.1101,  0.1458],\n",
      "         [ 0.1521, -0.0419,  0.1183,  ...,  0.2034, -0.0294,  0.0597]],\n",
      "\n",
      "        [[ 0.1873, -0.1055,  0.1771,  ...,  0.0905, -0.0394,  0.0839],\n",
      "         [-0.0291, -0.1144,  0.1975,  ...,  0.1841,  0.0303, -0.1692],\n",
      "         [ 0.2358,  0.1889,  0.0876,  ...,  0.2573, -0.0312,  0.3747],\n",
      "         ...,\n",
      "         [ 0.2598,  0.2223,  0.0264,  ...,  0.3005,  0.0209,  0.1886],\n",
      "         [ 0.1805,  0.1015,  0.0554,  ...,  0.2069, -0.1082,  0.1201],\n",
      "         [ 0.1633, -0.0114,  0.0999,  ...,  0.2165, -0.0111,  0.0334]],\n",
      "\n",
      "        [[ 0.1900, -0.1537,  0.1856,  ...,  0.0603, -0.0527,  0.1014],\n",
      "         [ 0.1259,  0.1436,  0.1733,  ...,  0.0057, -0.2073,  0.2822],\n",
      "         [ 0.1438,  0.1049,  0.1187,  ...,  0.3492,  0.0628,  0.3990],\n",
      "         ...,\n",
      "         [ 0.2403,  0.2184,  0.0509,  ...,  0.2703,  0.0136,  0.2606],\n",
      "         [ 0.1713,  0.1135,  0.0294,  ...,  0.1555, -0.0908,  0.1360],\n",
      "         [ 0.2055,  0.0109,  0.0910,  ...,  0.1889, -0.0086,  0.0674]],\n",
      "\n",
      "        [[ 0.1459, -0.1389,  0.2044,  ...,  0.0684, -0.0393,  0.0920],\n",
      "         [ 0.0292, -0.1299,  0.0607,  ...,  0.2153,  0.0557,  0.1453],\n",
      "         [ 0.2177,  0.1613,  0.1188,  ...,  0.2493, -0.0541,  0.3621],\n",
      "         ...,\n",
      "         [ 0.2424,  0.2013,  0.0597,  ...,  0.2560, -0.0110,  0.1853],\n",
      "         [ 0.1725,  0.0749,  0.0838,  ...,  0.1522, -0.1322,  0.1284],\n",
      "         [ 0.1861, -0.0271,  0.1098,  ...,  0.1776, -0.0192,  0.0362]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "masked_pos : tensor([[[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [15, 15, 15,  ..., 15, 15, 15],\n",
      "         [ 5,  5,  5,  ...,  5,  5,  5],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 7,  7,  7,  ...,  7,  7,  7],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[17, 17, 17,  ..., 17, 17, 17],\n",
      "         [ 6,  6,  6,  ...,  6,  6,  6],\n",
      "         [13, 13, 13,  ..., 13, 13, 13],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 6,  6,  6,  ...,  6,  6,  6],\n",
      "         [15, 15, 15,  ..., 15, 15, 15],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 5,  5,  5,  ...,  5,  5,  5],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 7,  7,  7,  ...,  7,  7,  7],\n",
      "         [14, 14, 14,  ..., 14, 14, 14],\n",
      "         [ 6,  6,  6,  ...,  6,  6,  6],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]])\n",
      "h_masked : tensor([[[ 0.1544, -0.1170,  0.0933,  ...,  0.2966, -0.1303,  0.2422],\n",
      "         [ 0.3820,  0.1522, -0.0011,  ...,  0.2502, -0.0807,  0.1834],\n",
      "         [ 0.0306, -0.1449,  0.0264,  ...,  0.2349, -0.0200,  0.0474],\n",
      "         [ 0.1532, -0.1147,  0.1736,  ...,  0.0703, -0.0467,  0.0740],\n",
      "         [ 0.1532, -0.1147,  0.1736,  ...,  0.0703, -0.0467,  0.0740]],\n",
      "\n",
      "        [[ 0.1965, -0.0043,  0.1919,  ...,  0.1520, -0.0971, -0.0237],\n",
      "         [ 0.0327, -0.1122,  0.1033,  ...,  0.0070,  0.1207,  0.0448],\n",
      "         [ 0.1928, -0.1364,  0.1355,  ...,  0.0802, -0.0259,  0.1209],\n",
      "         [ 0.1928, -0.1364,  0.1355,  ...,  0.0802, -0.0259,  0.1209],\n",
      "         [ 0.1928, -0.1364,  0.1355,  ...,  0.0802, -0.0259,  0.1209]],\n",
      "\n",
      "        [[ 0.2602,  0.0607,  0.1147,  ...,  0.3878, -0.1119,  0.2537],\n",
      "         [ 0.2622, -0.0146,  0.1336,  ...,  0.2204, -0.0253,  0.0501],\n",
      "         [ 0.4259,  0.2853, -0.0172,  ...,  0.2718, -0.0857,  0.1009],\n",
      "         [ 0.1910, -0.1339,  0.1811,  ...,  0.0929, -0.0430,  0.0806],\n",
      "         [ 0.1910, -0.1339,  0.1811,  ...,  0.0929, -0.0430,  0.0806]],\n",
      "\n",
      "        [[ 0.2678, -0.0261,  0.1139,  ...,  0.2240, -0.0305,  0.0271],\n",
      "         [ 0.4240,  0.1376,  0.0130,  ...,  0.2650, -0.0750,  0.1770],\n",
      "         [-0.0291, -0.1144,  0.1975,  ...,  0.1841,  0.0303, -0.1692],\n",
      "         [ 0.1873, -0.1055,  0.1771,  ...,  0.0905, -0.0394,  0.0839],\n",
      "         [ 0.1873, -0.1055,  0.1771,  ...,  0.0905, -0.0394,  0.0839]],\n",
      "\n",
      "        [[ 0.1438,  0.1049,  0.1187,  ...,  0.3492,  0.0628,  0.3990],\n",
      "         [ 0.0673, -0.1791,  0.0555,  ...,  0.2413, -0.0462,  0.0363],\n",
      "         [ 0.1900, -0.1537,  0.1856,  ...,  0.0603, -0.0527,  0.1014],\n",
      "         [ 0.1900, -0.1537,  0.1856,  ...,  0.0603, -0.0527,  0.1014],\n",
      "         [ 0.1900, -0.1537,  0.1856,  ...,  0.0603, -0.0527,  0.1014]],\n",
      "\n",
      "        [[ 0.1543, -0.0561,  0.2342,  ...,  0.1274, -0.1003, -0.0380],\n",
      "         [ 0.3238,  0.1427, -0.0343,  ...,  0.1199, -0.3366,  0.3348],\n",
      "         [ 0.2326, -0.0738,  0.1331,  ...,  0.1901, -0.0496,  0.0290],\n",
      "         [ 0.1459, -0.1389,  0.2044,  ...,  0.0684, -0.0393,  0.0920],\n",
      "         [ 0.1459, -0.1389,  0.2044,  ...,  0.0684, -0.0393,  0.0920]]],\n",
      "       grad_fn=<GatherBackward0>)\n",
      "shape of h_masked : torch.Size([6, 5, 768])\n",
      "shape of logits_lm : torch.Size([6, 5, 29])\n",
      "logits_lm : tensor([-55.9029,  -7.7836,  12.0027, -11.3412,  13.4980, -16.8324,   8.7980,\n",
      "         13.3922,  18.6503,   2.5167], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch(batch_size=batch_size,sentences=sentences,token_list=token_list,word_dict=word_dict,max_pred=max_pred,maxlen=maxlen,vocab_size=vocab_size,number_dict=number_dict)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "\n",
    "\n",
    "# embdding layer\n",
    "embedding = Embedding(vocab_size=vocab_size,d_model=d_model,maxlen=maxlen,n_segments=n_segments)\n",
    "\n",
    "# embedding output\n",
    "output = embedding(input_ids, segment_ids)\n",
    "print(f'embedding shape:{output.shape}')\n",
    "\n",
    "# padding of the key, query , value\n",
    "enc_self_attn_mask = get_attn_pad_mask(seq_q=input_ids,seq_k=input_ids)\n",
    "\n",
    "# 6 encoder layers\n",
    "layers = nn.ModuleList([EncoderLayer(d_model=d_model,d_k=d_k,n_heads=n_heads,d_v=d_v,d_ff=d_ff) for _ in range(n_layers)])\n",
    "\n",
    "# ouput of encoder layers\n",
    "# inputs are output : embedding ouput , enc_self_attn_mask = padding mask of sentences\n",
    "output_list = []\n",
    "for layer in layers:\n",
    "    output,enc_self_attn = layer(output,enc_self_attn_mask)\n",
    "    output_list.append([output,enc_self_attn])\n",
    "\n",
    "print(output.shape,enc_self_attn.shape)\n",
    "print(f\"ouput of encoder : {output.shape} \")\n",
    "print(f\"ouput of enc_self_attn : {enc_self_attn.shape} \")\n",
    "# -------------------------------------------------------------\n",
    "fc = nn.Linear(d_model, d_model)\n",
    "activ1 = nn.Tanh()\n",
    "linear = nn.Linear(d_model, d_model)\n",
    "activ2 = gelu\n",
    "norm = nn.LayerNorm(d_model)\n",
    "classifier = nn.Linear(d_model, 2)\n",
    "# decoder is shared with embedding layer\n",
    "\n",
    "\n",
    "embed_weight = embedding.tok_embed.weight\n",
    "print(f\"embedding weight shape : {embed_weight.shape}\")\n",
    "n_vocab, n_dim = embed_weight.size()\n",
    "print(f\"n_vocab ,n_dim : {n_vocab,n_dim}\")\n",
    "\n",
    "# making linear layer for decoder which maps\n",
    "# n_dim -> n_vocab\n",
    "decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "\n",
    "# decoder weight is not learned but asigned with the token embedding\n",
    "# so that it uses the pupose of mapping instead of learning eachtime\n",
    "decoder.weight = embed_weight\n",
    "\n",
    "is_share_same = decoder.weight.storage().data_ptr() == embed_weight.storage().data_ptr() \n",
    "print(f\"decoder weight and embedding weight share same memory: {is_share_same}\")\n",
    "# using decoder bias as 0\n",
    "decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "# ---------------------------forward ----------------------\n",
    "# output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_model, d_model]\n",
    "# it will be decided by first token(CLS)\n",
    "print(f\"shape of output: {output.shape}\")\n",
    "\n",
    "# getting first column of all n_batch , with first word and all d_model\n",
    "print(f\"shape of output[:,0] : {output[:,0].shape}\")\n",
    "\n",
    "h_pooled = activ1(fc(output[:, 0])) # [batch_size, d_model]\n",
    "print(f\" shape of h_pooled : {h_pooled.shape}\")\n",
    "\n",
    "# taking only  2 for classification purpose\n",
    "logits_clsf = classifier(h_pooled) # [batch_size, 2]\n",
    "print(f\" shape of logits_clsf : {logits_clsf.shape}\")\n",
    "print(f\"logits_clsf : {logits_clsf}\")\n",
    "\n",
    "# -------------------------------masked_pos --------------------------\n",
    "\n",
    "\n",
    "\n",
    "masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "print(f\" shape of masked_pos: {masked_pos.shape}\")\n",
    "# get masked position from final output of transformer.\n",
    "print(f\"shape of final output: {output.shape}\")\n",
    "print(f\"output : {output}\")\n",
    "print(f\"masked_pos : {masked_pos}\")\n",
    "\n",
    "# https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "print(f\"h_masked : {h_masked}\")\n",
    "print(f\"shape of h_masked : {h_masked.shape}\")\n",
    "# h_masked contains the masked value of all the positions that is masked \n",
    "# returns only the corresponding max_prediction with d_model dimention since each mask dimention is to be masked\n",
    "\n",
    "\n",
    "h_masked = norm(activ2(linear(h_masked)))\n",
    "\n",
    "# -----------------------------------------masked position prediction ------------------------\n",
    "logits_lm = decoder(h_masked) + decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "print(f\"shape of logits_lm : {logits_lm.shape}\")\n",
    "\n",
    "# logits_lm and logits_clsf are returned\n",
    "\n",
    "print(f\"logits_lm : {logits_lm[0,0,:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_lm transpose : torch.Size([6, 29, 5]) \n",
      "masked_tokens shape : torch.Size([6, 5])\n",
      "logits_lm_tran : tensor([[[-2.0260e+01, -2.8884e+01, -3.2608e+01,  3.5535e+00,  3.5535e+00],\n",
      "         [-5.3974e+01, -2.5572e+01, -5.0938e+01, -3.4439e+01, -3.4439e+01],\n",
      "         [ 1.6337e+01,  2.0226e+01,  2.4240e+01, -1.3174e+01, -1.3174e+01],\n",
      "         [ 5.3234e+01,  2.2370e+01,  5.1465e+01,  6.0208e+01,  6.0208e+01],\n",
      "         [-4.0959e+01, -2.9181e+01, -2.9680e+01, -4.8465e+01, -4.8465e+01],\n",
      "         [ 2.6439e+00,  1.1836e+01,  4.2148e+00,  1.6693e+01,  1.6693e+01],\n",
      "         [ 1.4258e+01,  4.1194e+01,  4.6380e+01,  2.1247e+01,  2.1247e+01],\n",
      "         [ 4.1309e-02,  2.2970e+01,  1.7018e+01, -6.1084e-02, -6.1084e-02],\n",
      "         [-1.8357e+01,  8.7911e+00,  3.0643e+01, -2.8595e+00, -2.8595e+00],\n",
      "         [ 5.9858e+01,  6.6744e+01,  5.9283e+01,  2.4697e+01,  2.4697e+01],\n",
      "         [ 1.9206e+01,  3.0109e+01,  2.1948e+01,  2.5134e+01,  2.5134e+01],\n",
      "         [ 3.3579e+01, -1.6922e+00,  3.5415e+01,  1.6127e+01,  1.6127e+01],\n",
      "         [-3.1829e+01,  1.2192e+00, -1.3166e+01, -7.9385e+00, -7.9385e+00],\n",
      "         [ 1.1054e+01,  1.5896e+01,  2.0802e+01, -2.3824e+01, -2.3824e+01],\n",
      "         [ 3.0800e+00,  2.7090e+01, -1.7756e+01, -1.6098e+01, -1.6098e+01],\n",
      "         [ 2.2484e+00, -2.1679e+01, -1.1769e+01, -4.4336e+01, -4.4336e+01],\n",
      "         [-3.0847e+01, -1.3215e+01, -1.8377e+01,  2.6779e+00,  2.6779e+00],\n",
      "         [-9.0036e+00, -1.8155e+01, -1.7848e+01, -3.2446e+01, -3.2446e+01],\n",
      "         [-8.2709e+00,  1.1315e+01, -1.6016e+01,  9.3091e+00,  9.3091e+00],\n",
      "         [-2.7468e+00,  2.4520e+01,  1.8898e+00,  7.2203e+00,  7.2203e+00],\n",
      "         [ 1.1183e+01,  2.0180e+00,  4.1951e+01, -1.6418e+01, -1.6418e+01],\n",
      "         [ 1.3561e+01, -1.0908e+01,  9.6340e+00,  4.1549e+01,  4.1549e+01],\n",
      "         [-2.2064e+00, -2.3808e+01, -1.0598e+01, -8.1326e+00, -8.1326e+00],\n",
      "         [ 5.3409e+00,  2.8880e+01, -1.4746e+01, -7.7834e+00, -7.7834e+00],\n",
      "         [ 9.0090e+00,  3.6444e+01,  1.8853e+01,  1.8196e+01,  1.8196e+01],\n",
      "         [-4.0896e+01,  4.8655e+00, -3.4339e+01, -2.5208e+01, -2.5208e+01],\n",
      "         [-3.2964e+01, -2.6817e+01, -5.1028e+01, -6.5266e+01, -6.5266e+01],\n",
      "         [ 1.0795e+00, -4.1537e+01,  1.1869e+01,  5.0677e+01,  5.0677e+01],\n",
      "         [-1.3602e+01,  4.8062e-01, -1.5519e+01,  7.0157e+00,  7.0157e+00]],\n",
      "\n",
      "        [[ 4.7337e+00, -1.2052e+01, -1.9945e+01,  5.1449e+00,  5.1449e+00],\n",
      "         [-5.4617e+01, -4.8728e+01, -3.2491e+00, -2.8442e+01, -2.8442e+01],\n",
      "         [ 2.9300e+01, -3.7476e+01, -1.2555e+01, -1.8743e+01, -1.8743e+01],\n",
      "         [ 1.1138e+01,  4.7717e+01,  2.8006e+01,  5.0129e+01,  5.0129e+01],\n",
      "         [-1.3765e+01, -4.3303e+01, -3.9493e+01, -4.4804e+01, -4.4804e+01],\n",
      "         [ 2.5711e+01,  1.7098e+01,  1.3088e+01,  2.0071e+01,  2.0071e+01],\n",
      "         [ 2.1521e+01,  2.3958e+01,  2.6224e+01,  2.3343e+01,  2.3343e+01],\n",
      "         [-3.7286e+01, -2.5524e+01,  3.3137e+01,  2.3433e+00,  2.3433e+00],\n",
      "         [ 9.8645e-01, -1.7780e+01, -1.4266e+00, -2.3691e+00, -2.3691e+00],\n",
      "         [ 4.3007e+01,  3.4658e+01,  6.2298e+01,  2.5843e+01,  2.5843e+01],\n",
      "         [ 2.7993e+01,  4.1012e+01, -2.6951e-01,  2.4834e+01,  2.4834e+01],\n",
      "         [ 3.5398e+01,  1.8056e+01,  5.7955e+00,  2.3730e+01,  2.3730e+01],\n",
      "         [ 1.2605e+01, -1.2990e+01,  2.8811e+00, -7.5134e+00, -7.5134e+00],\n",
      "         [-1.9464e+01,  7.8557e+00,  5.6234e+01, -1.8825e+01, -1.8825e+01],\n",
      "         [ 5.9047e+01, -1.0109e+01,  2.2577e+01, -1.4160e+01, -1.4160e+01],\n",
      "         [-4.4660e+01, -3.4289e+00, -2.7977e+01, -4.7657e+01, -4.7657e+01],\n",
      "         [-1.8155e+01, -1.3221e+01, -3.0585e+01,  1.3367e+00,  1.3367e+00],\n",
      "         [-1.4097e+01, -1.7609e+01,  5.9117e+00, -4.0124e+01, -4.0124e+01],\n",
      "         [-1.6662e+01, -2.1524e+01, -3.1295e+01,  8.1173e+00,  8.1173e+00],\n",
      "         [ 5.1226e+00, -7.0618e+00,  2.9123e+01,  1.3374e+01,  1.3374e+01],\n",
      "         [ 6.0151e+00,  1.9172e+01, -9.3383e+00, -1.4781e+01, -1.4781e+01],\n",
      "         [-1.3261e+01, -6.3910e-01, -4.8479e+00,  4.0568e+01,  4.0568e+01],\n",
      "         [ 1.8697e+01, -8.6053e+00, -5.1561e+01, -1.2908e+01, -1.2908e+01],\n",
      "         [ 1.4663e+01, -2.0877e+01, -4.0243e+01, -5.6658e+00, -5.6658e+00],\n",
      "         [ 3.6334e+01,  1.1205e+01, -1.7534e+01,  1.7893e+01,  1.7893e+01],\n",
      "         [ 4.7645e+00, -4.7762e+01, -2.9180e+01, -3.1241e+01, -3.1241e+01],\n",
      "         [-9.1893e+01, -3.6823e+01, -6.2480e+01, -6.6831e+01, -6.6831e+01],\n",
      "         [-1.0851e+01, -7.0337e+00, -4.5427e+01,  4.1799e+01,  4.1799e+01],\n",
      "         [-9.1452e+00,  9.1237e+00,  2.4439e+01,  6.0526e+00,  6.0526e+00]],\n",
      "\n",
      "        [[-4.2026e+00, -1.7207e+01, -1.3365e+01,  4.1126e+00,  4.1126e+00],\n",
      "         [-1.2525e+01, -3.1060e+01, -1.9301e+01, -3.3123e+01, -3.3123e+01],\n",
      "         [ 3.3453e+01,  2.6184e+01,  3.2796e+01, -1.2434e+01, -1.2434e+01],\n",
      "         [ 5.8365e+00,  4.4412e+01,  5.0091e+01,  5.5900e+01,  5.5900e+01],\n",
      "         [-3.2411e+01, -3.0333e+01, -4.2973e+01, -4.8202e+01, -4.8202e+01],\n",
      "         [ 9.3887e+00, -6.7820e+00,  3.6958e+01,  1.6722e+01,  1.6722e+01],\n",
      "         [ 4.8798e+00,  2.6489e+01,  3.1518e+01,  2.1840e+01,  2.1840e+01],\n",
      "         [ 3.7922e+01,  3.7662e+01,  1.8539e+01,  2.6256e+00,  2.6256e+00],\n",
      "         [-1.9785e+01, -5.1670e+01,  3.4218e+00, -3.4891e+00, -3.4891e+00],\n",
      "         [ 5.5092e+01,  5.7978e+01,  4.9609e+01,  3.0256e+01,  3.0256e+01],\n",
      "         [ 2.4353e+01, -1.2431e+01,  2.0258e+01,  2.8455e+01,  2.8455e+01],\n",
      "         [ 2.3456e+01,  1.8179e+01,  6.7084e+00,  1.8463e+01,  1.8463e+01],\n",
      "         [ 1.1011e+01, -4.4184e+00,  1.0458e+01, -3.0864e+00, -3.0864e+00],\n",
      "         [ 2.5224e+01,  8.4609e+00,  3.1013e+01, -2.2616e+01, -2.2616e+01],\n",
      "         [ 1.1466e+01,  2.9088e+01,  4.2521e+00, -1.4292e+01, -1.4292e+01],\n",
      "         [-2.7948e+01, -3.0148e+01, -4.7989e+00, -4.3790e+01, -4.3790e+01],\n",
      "         [-4.1285e+01, -4.9176e+01, -2.5088e+01,  8.0513e-01,  8.0513e-01],\n",
      "         [-3.0958e+01, -2.6947e+01, -5.1602e+01, -3.4820e+01, -3.4820e+01],\n",
      "         [-1.0891e+01,  9.1276e+00, -5.8666e-01,  7.3894e+00,  7.3894e+00],\n",
      "         [ 2.7212e+01,  3.5684e+01,  2.8086e+01,  7.6001e+00,  7.6001e+00],\n",
      "         [-3.1997e+00,  2.9962e+01,  1.2286e+01, -1.5716e+01, -1.5716e+01],\n",
      "         [ 7.8586e+00,  7.6228e+00, -1.5998e+01,  4.2514e+01,  4.2514e+01],\n",
      "         [ 5.7984e-01,  5.7058e+00,  2.5465e+00, -9.1236e+00, -9.1236e+00],\n",
      "         [ 1.0729e+01,  1.1960e+00,  9.2751e+00, -7.1386e+00, -7.1386e+00],\n",
      "         [ 3.3656e+01,  2.0059e+01,  1.8995e+01,  1.3922e+01,  1.3922e+01],\n",
      "         [-7.9291e+00, -2.6285e+01,  1.2129e+01, -2.7035e+01, -2.7035e+01],\n",
      "         [-2.2414e+01, -3.9660e+01, -2.8334e+01, -6.7029e+01, -6.7029e+01],\n",
      "         [-3.7282e+01,  5.2343e+00, -3.1915e+01,  4.6667e+01,  4.6667e+01],\n",
      "         [-1.4679e+01,  1.4368e+01, -1.6343e+01,  2.7478e-01,  2.7478e-01]],\n",
      "\n",
      "        [[-3.2555e+00, -2.1689e+01,  6.2133e+00,  6.2133e+00,  6.2133e+00],\n",
      "         [-1.7537e+01, -3.0035e+01, -3.0739e+01, -3.0739e+01, -3.0739e+01],\n",
      "         [ 2.9411e+01,  1.9740e+01, -1.2608e+01, -1.2608e+01, -1.2608e+01],\n",
      "         [-8.4786e-01,  2.7165e+01,  4.7408e+01,  4.7408e+01,  4.7408e+01],\n",
      "         [-3.2157e+01, -3.0524e+01, -4.6671e+01, -4.6671e+01, -4.6671e+01],\n",
      "         [ 3.8204e+00, -4.8868e+00,  1.7955e+01,  1.7955e+01,  1.7955e+01],\n",
      "         [-1.1900e+00,  2.4259e+01,  1.7215e+01,  1.7215e+01,  1.7215e+01],\n",
      "         [ 3.3314e+01,  1.5814e+01,  1.2882e+00,  1.2882e+00,  1.2882e+00],\n",
      "         [-2.0831e+01, -1.9038e+00, -6.8123e+00, -6.8123e+00, -6.8123e+00],\n",
      "         [ 5.3257e+01,  5.0766e+01,  2.8556e+01,  2.8556e+01,  2.8556e+01],\n",
      "         [ 2.6111e+01,  4.4108e+01,  2.9423e+01,  2.9423e+01,  2.9423e+01],\n",
      "         [ 2.0573e+01,  1.4047e+01,  2.1332e+01,  2.1332e+01,  2.1332e+01],\n",
      "         [ 1.5400e+01, -3.7296e+00, -3.3568e+00, -3.3568e+00, -3.3568e+00],\n",
      "         [ 2.5573e+01,  9.6923e+00, -1.8735e+01, -1.8735e+01, -1.8735e+01],\n",
      "         [ 1.0259e+01,  1.3228e+01, -1.7194e+01, -1.7194e+01, -1.7194e+01],\n",
      "         [-2.8254e+01, -1.1334e+01, -4.2524e+01, -4.2524e+01, -4.2524e+01],\n",
      "         [-4.0760e+01, -1.3630e+01,  4.0923e+00,  4.0923e+00,  4.0923e+00],\n",
      "         [-2.9764e+01, -3.0487e+00, -3.2512e+01, -3.2512e+01, -3.2512e+01],\n",
      "         [-1.2568e+01, -1.0733e+01,  5.7797e+00,  5.7797e+00,  5.7797e+00],\n",
      "         [ 2.4234e+01, -2.7906e+00,  2.4619e+00,  2.4619e+00,  2.4619e+00],\n",
      "         [ 1.1319e-01,  3.5130e+01, -1.9411e+01, -1.9411e+01, -1.9411e+01],\n",
      "         [ 6.2798e+00,  5.4791e+00,  4.3272e+01,  4.3272e+01,  4.3272e+01],\n",
      "         [-3.6169e+00, -8.5163e+00, -8.2498e+00, -8.2498e+00, -8.2498e+00],\n",
      "         [ 1.7982e+01,  1.0502e+01, -6.8680e+00, -6.8680e+00, -6.8680e+00],\n",
      "         [ 3.4400e+01, -6.8105e+00,  1.2727e+01,  1.2727e+01,  1.2727e+01],\n",
      "         [-8.2844e+00, -5.1070e+01, -1.7849e+01, -1.7849e+01, -1.7849e+01],\n",
      "         [-2.2138e+01, -1.7852e+01, -6.4647e+01, -6.4647e+01, -6.4647e+01],\n",
      "         [-3.8322e+01,  1.3789e+01,  4.8515e+01,  4.8515e+01,  4.8515e+01],\n",
      "         [-7.6963e+00, -2.6738e+01,  9.0916e+00,  9.0916e+00,  9.0916e+00]],\n",
      "\n",
      "        [[-8.2470e+00, -2.8390e+01, -3.2740e+01,  5.0963e+00,  5.0963e+00],\n",
      "         [-1.5206e+01, -4.7518e+01, -4.9181e+01, -2.9317e+01, -2.9317e+01],\n",
      "         [-1.6647e+01,  3.6589e+01,  2.0760e+01, -1.3080e+01, -1.3080e+01],\n",
      "         [ 3.3151e+01,  2.3927e+01,  4.6615e+01,  5.2333e+01,  5.2333e+01],\n",
      "         [-4.5841e+01, -2.5632e+01, -2.9880e+01, -4.8866e+01, -4.8866e+01],\n",
      "         [ 3.0986e+00, -1.6450e+00,  6.4597e+00,  1.7402e+01,  1.7402e+01],\n",
      "         [ 3.8645e+01,  1.8988e+01,  4.6299e+01,  1.8645e+01,  1.8645e+01],\n",
      "         [ 9.9623e+00,  3.0304e+01,  2.0179e+01,  4.8371e+00,  4.8371e+00],\n",
      "         [ 1.2912e+01, -1.7866e+01,  3.2019e+01,  1.0534e+00,  1.0534e+00],\n",
      "         [ 3.6595e+01,  3.7831e+01,  5.9781e+01,  2.5544e+01,  2.5544e+01],\n",
      "         [ 2.5514e+01, -8.9698e+00,  2.5531e+01,  2.6653e+01,  2.6653e+01],\n",
      "         [ 1.4790e+01,  2.0260e+01,  4.0700e+01,  2.5199e+01,  2.5199e+01],\n",
      "         [-1.4093e+01,  4.7729e+00, -1.4337e+01, -5.4824e+00, -5.4824e+00],\n",
      "         [-6.1213e+00,  2.4787e+01,  2.1512e+01, -2.2637e+01, -2.2637e+01],\n",
      "         [ 8.8137e+00,  3.5805e+01, -1.8910e+01, -1.9428e+01, -1.9428e+01],\n",
      "         [ 9.7805e-01, -2.5157e+01, -1.1053e+01, -4.5057e+01, -4.5057e+01],\n",
      "         [-3.7382e+01, -3.1338e+01, -1.5504e+01,  2.4965e+00,  2.4965e+00],\n",
      "         [ 3.4183e+00, -1.9998e+01, -1.8190e+01, -3.5167e+01, -3.5167e+01],\n",
      "         [-1.7196e+01, -6.3939e+00, -1.6404e+01,  8.0274e+00,  8.0274e+00],\n",
      "         [ 3.7598e+00,  3.0665e+01, -1.4943e+00,  5.8718e+00,  5.8718e+00],\n",
      "         [ 1.1552e+01,  2.4585e+01,  4.3078e+01, -1.1791e+01, -1.1791e+01],\n",
      "         [ 1.6010e+01, -7.2633e+00,  6.4074e+00,  3.9789e+01,  3.9789e+01],\n",
      "         [-7.9545e+00,  1.3250e+01, -1.0658e+01, -4.6948e+00, -4.6948e+00],\n",
      "         [ 3.4302e+00,  2.9422e+01, -1.1482e+01, -7.0352e+00, -7.0352e+00],\n",
      "         [ 4.1643e+01,  2.3371e+01,  1.6227e+01,  1.3857e+01,  1.3857e+01],\n",
      "         [-3.3851e+01, -2.7872e+01, -3.6110e+01, -3.0264e+01, -3.0264e+01],\n",
      "         [-1.1817e+01, -3.2428e+01, -5.3956e+01, -6.6281e+01, -6.6281e+01],\n",
      "         [-1.9819e+01, -3.5590e+00,  4.6351e+00,  4.6641e+01,  4.6641e+01],\n",
      "         [-2.9725e+01, -8.8618e+00, -1.9117e+01,  3.0525e+00,  3.0525e+00]],\n",
      "\n",
      "        [[-1.0748e+01, -2.0815e+01, -9.2094e+00,  5.2845e+00,  5.2845e+00],\n",
      "         [-4.9041e+01, -3.1278e+01, -1.7413e+01, -3.0285e+01, -3.0285e+01],\n",
      "         [-3.7432e+01,  1.8104e+01, -3.5607e+01, -1.5471e+01, -1.5471e+01],\n",
      "         [ 4.9842e+01,  2.9472e+01,  2.4767e+01,  5.1007e+01,  5.1007e+01],\n",
      "         [-4.3317e+01, -2.5655e+01, -4.7381e+01, -4.6096e+01, -4.6096e+01],\n",
      "         [ 1.6740e+01, -2.1993e+00,  1.4351e+01,  1.9001e+01,  1.9001e+01],\n",
      "         [ 2.0896e+01,  3.7156e+01,  1.9955e+01,  2.0462e+01,  2.0462e+01],\n",
      "         [-2.2201e+01,  1.5549e+01, -1.3412e+01,  4.0943e+00,  4.0943e+00],\n",
      "         [-1.9661e+01,  5.1420e+00, -3.5338e+00,  1.1481e+00,  1.1481e+00],\n",
      "         [ 3.3417e+01,  4.5640e+01,  3.0498e+01,  2.4427e+01,  2.4427e+01],\n",
      "         [ 3.8848e+01,  3.0971e+01,  1.5190e+01,  2.3593e+01,  2.3593e+01],\n",
      "         [ 1.9594e+01,  1.5658e+01,  1.9651e+01,  2.2054e+01,  2.2054e+01],\n",
      "         [-1.2050e+01, -7.3999e+00, -2.2029e+01, -9.7274e+00, -9.7274e+00],\n",
      "         [ 8.6028e+00,  8.9599e+00,  3.2387e+01, -1.8855e+01, -1.8855e+01],\n",
      "         [-1.1935e+01,  9.7294e+00,  2.8490e+00, -2.0493e+01, -2.0493e+01],\n",
      "         [-3.3688e+00, -7.3321e+00,  1.3935e+01, -4.4017e+01, -4.4017e+01],\n",
      "         [-1.4750e+01, -1.3471e+01, -8.1801e+00,  4.6589e+00,  4.6589e+00],\n",
      "         [-2.0178e+01, -3.2901e-01,  3.3997e+00, -3.4865e+01, -3.4865e+01],\n",
      "         [-2.1580e+01, -1.3795e+01, -1.8914e+01,  5.5930e+00,  5.5930e+00],\n",
      "         [-9.3357e+00,  1.6079e+00, -1.2358e+01,  7.9914e+00,  7.9914e+00],\n",
      "         [ 2.0130e+01,  3.7028e+01,  3.2449e+01, -1.7610e+01, -1.7610e+01],\n",
      "         [-3.2491e-01,  4.7811e+00,  2.4532e+01,  3.9625e+01,  3.9625e+01],\n",
      "         [-3.3005e+00, -5.5509e+00, -8.6740e+00, -2.9386e+00, -2.9386e+00],\n",
      "         [-2.0598e+01,  8.5770e+00, -2.7432e+00, -5.6264e+00, -5.6264e+00],\n",
      "         [ 1.0029e+01, -6.3596e+00, -1.7711e+01,  1.8601e+01,  1.8601e+01],\n",
      "         [-4.9854e+01, -5.0563e+01, -4.1248e+01, -2.5524e+01, -2.5524e+01],\n",
      "         [-3.4461e+01, -2.3613e+01, -2.5397e+01, -6.4544e+01, -6.4544e+01],\n",
      "         [ 1.1182e-01,  1.6680e+01, -3.1399e+01,  5.1210e+01,  5.1210e+01],\n",
      "         [ 1.0169e+01, -1.5800e+01, -1.4852e+01,  8.5223e+00,  8.5223e+00]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "masked_tokens : tensor([[ 5, 22, 23,  0,  0],\n",
      "        [ 8, 11, 21,  0,  0],\n",
      "        [ 5, 19, 18,  0,  0],\n",
      "        [26, 10,  0,  0,  0],\n",
      "        [ 4,  5, 12,  0,  0],\n",
      "        [24, 11, 15,  0,  0]])\n",
      "loss_lm before mean: 49.685359954833984\n",
      "loss_clsf logits : torch.Size([6, 2])\n",
      "isNext : torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch(batch_size=batch_size,sentences=sentences,token_list=token_list,word_dict=word_dict,max_pred=max_pred,maxlen=maxlen,vocab_size=vocab_size,number_dict=number_dict)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "\n",
    "# criterion\n",
    "model = BERT(d_model=d_model,n_layers=n_layers,vocab_size=vocab_size,d_k=d_k,d_v=d_v,n_heads=n_heads,d_ff=d_ff)\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm_tran = logits_lm.transpose(1,2)\n",
    "masked_tokens = masked_tokens\n",
    "\n",
    "print(f\"logits_lm transpose : {logits_lm_tran.shape} \")\n",
    "# logits_lm transpose : shape( n_batch , vocab_position , mask_pos_output)\n",
    "\n",
    "print(f\"masked_tokens shape : {masked_tokens.shape}\")\n",
    "\n",
    "print(f\"logits_lm_tran : {logits_lm_tran}\")\n",
    "# token means vocab position\n",
    "print(f\"masked_tokens : {masked_tokens}\")\n",
    "\n",
    "loss_lm = criterion(logits_lm_tran, masked_tokens) # for masked LM\n",
    "\n",
    "loss_lm = (loss_lm.float()).mean()\n",
    "\n",
    "print(f\"loss_lm before mean: {loss_lm}\")\n",
    "\n",
    "print(f\"loss_clsf logits : {logits_clsf.shape}\")\n",
    "print(f\"isNext : {isNext.shape}\")\n",
    "\n",
    "loss_clsf = criterion(logits_clsf, isNext) \n",
    "\n",
    "# objective function of loss\n",
    "loss = loss_lm + loss_clsf\n",
    "\n",
    "loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n",
      "['[CLS]', '[MASK]', 'meet', 'you', 'too', 'how', 'are', 'you', 'today', '[SEP]', 'great', 'my', 'baseball', 'team', 'won', '[MASK]', 'competition', '[SEP]']\n",
      "masked tokens list :  [15, 11, 28]\n",
      "predict masked tokens list :  [22, 3, 22, 21, 21]\n",
      "logits_clsf : [1]\n",
      "isNext :  True\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict mask tokens ans isNext\n",
    "# passing first sentence on;\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "#-------------------------------------\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "\n",
    "# obtaining the masked token list and ignoring the \"0\" valued tokens\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "\n",
    "# obtraining the masked token from the prediction and ignoring the \"0\" value tokens\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "# obtraining the classification prediction label \n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()\n",
    "print(f\"logits_clsf : {logits_clsf}\")\n",
    "logits_clsf = logits_clsf[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'meet', 'you', 'too', 'how', 'are', 'you', 'today', '[SEP]', 'great', 'my', 'baseball', 'team', 'won', '[MASK]', 'competition', '[SEP]']\n",
      "masked lm shape: torch.Size([1, 5, 29])\n",
      "masked lm shape: torch.Size([1, 5])\n",
      "masked lm shape: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# what is happening to the logits_lm\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "# print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "print(f\"masked lm shape: {logits_lm.shape}\")\n",
    "\n",
    "# accessing the 2nd row of max of 29 vocab token from the row of 5 see how max works\n",
    "logits_lm = logits_lm.data.max(2)[1]\n",
    "print(f\"masked lm shape: {logits_lm.shape}\")\n",
    "\n",
    "# accessing the 1st row from batches\n",
    "logits_lm = logits_lm[0]\n",
    "print(f\"masked lm shape: {logits_lm.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim0 max in dimension 0 : torch.return_types.max(\n",
      "values=tensor([[ 10, 100,  23],\n",
      "        [ 23,  30,  20],\n",
      "        [100,  30,  43]]),\n",
      "indices=tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [0, 1, 1]])) \n",
      "\n",
      "dim1 max in dimension 1 : torch.return_types.max(\n",
      "values=tensor([[100,   4,   7],\n",
      "        [ 34, 100,  43]]),\n",
      "indices=tensor([[2, 1, 2],\n",
      "        [2, 0, 2]])) \n",
      "\n",
      "dim2 max in dimension 2 : torch.return_types.max(\n",
      "values=tensor([[  3,   4, 100],\n",
      "        [100,  30,  43]]),\n",
      "indices=tensor([[2, 1, 0],\n",
      "        [1, 1, 2]])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how max behaves \n",
    "\n",
    "test1 = [\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [2,4,0],\n",
    "        [100,2,7],\n",
    "    ],\n",
    "    [\n",
    "        [10,100,23],\n",
    "        [23,30,20],\n",
    "        [34,30,43],\n",
    "    ]\n",
    "]\n",
    "\n",
    "dim0 =  [# v v v row 1 #  v v v row 2#  v v v row 3 indexing this way and also scanning this way\n",
    "    [[1,2,3],[2,4,0],[100,2,7],],\n",
    "    [[10,100,23],[23,30,20],[34,30,43],]\n",
    "]\n",
    "\n",
    "dim1 = [\n",
    "    [  # V V V  1st scan and store in 1st row of 2d matrix and indexing is done top to bottom\n",
    "        [1,2,3],\n",
    "        [2,4,0],\n",
    "        [100,2,7],\n",
    "    ],\n",
    "    [ # v v v    2nd scan and store in 2nd row of 2d matrix\n",
    "        [10,100,23],\n",
    "        [23,30,20],\n",
    "        [34,30,43],\n",
    "    ]\n",
    "]\n",
    "\n",
    "dim2 = [\n",
    "    [ \n",
    "        [1,2,3], # <\n",
    "        [2,4,0], # <    scan this way and store in 1st row of 2d matrix and index is done left to right\n",
    "        [100,2,7], # <\n",
    "    ],\n",
    "    [\n",
    "        [10,100,23], # <\n",
    "        [23,30,20],  # <  scan this way and store in 2nd row of 2d matrix\n",
    "        [34,30,43],  # < \n",
    "    ]\n",
    "]\n",
    "\n",
    "dims = {\n",
    "    'dim0': dim0,\n",
    "    'dim1': dim1,\n",
    "    \"dim2\": dim2\n",
    "}\n",
    "for i in range(3):\n",
    "    dimi = f'dim{i}'\n",
    "    dimi = dims[dimi]\n",
    "\n",
    "    dimi = torch.tensor(dimi)\n",
    "    \n",
    "    dimi_max = dimi.max(i)\n",
    "    print(f\"dim{i} max in dimension {i} : {dimi_max} \\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3,   4, 100])\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,3):\n",
    "    dimi = f'dim{i}'\n",
    "    dimi = dims[dimi]\n",
    "\n",
    "    dimi = torch.tensor(dimi)\n",
    "    \n",
    "    dimi_max = dimi.max(i)\n",
    "    print(dimi_max[0][0])\n",
    "    # print(f\"dim{i} max in dimension {i} : {dimi_max} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# masked token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked pos: tensor([[ 2,  1, 15,  0,  0],\n",
      "        [ 9,  4,  0,  0,  0],\n",
      "        [ 3,  2,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0],\n",
      "        [ 7,  2,  5,  0,  0],\n",
      "        [ 2,  1,  0,  0,  0]])\n",
      "masked_pos shape: torch.Size([6, 5])\n",
      "masked pos after unsqueeze: torch.Size([6, 5, 1])\n",
      "masked pos: tensor([[[ 2],\n",
      "         [ 1],\n",
      "         [15],\n",
      "         [ 0],\n",
      "         [ 0]],\n",
      "\n",
      "        [[ 9],\n",
      "         [ 4],\n",
      "         [ 0],\n",
      "         [ 0],\n",
      "         [ 0]],\n",
      "\n",
      "        [[ 3],\n",
      "         [ 2],\n",
      "         [ 0],\n",
      "         [ 0],\n",
      "         [ 0]],\n",
      "\n",
      "        [[ 7],\n",
      "         [ 0],\n",
      "         [ 0],\n",
      "         [ 0],\n",
      "         [ 0]],\n",
      "\n",
      "        [[ 7],\n",
      "         [ 2],\n",
      "         [ 5],\n",
      "         [ 0],\n",
      "         [ 0]],\n",
      "\n",
      "        [[ 2],\n",
      "         [ 1],\n",
      "         [ 0],\n",
      "         [ 0],\n",
      "         [ 0]]])\n",
      "masked pos after expanding: torch.Size([6, 5, 768])\n",
      "masked pos: tensor([[[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [15, 15, 15,  ..., 15, 15, 15],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 9,  9,  9,  ...,  9,  9,  9],\n",
      "         [ 4,  4,  4,  ...,  4,  4,  4],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 7,  7,  7,  ...,  7,  7,  7],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 7,  7,  7,  ...,  7,  7,  7],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 5,  5,  5,  ...,  5,  5,  5],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "# what  the masked_pos is doing?\n",
    "batch = make_batch(batch_size=batch_size,sentences=sentences,token_list=token_list,word_dict=word_dict,max_pred=max_pred,maxlen=maxlen,vocab_size=vocab_size,number_dict=number_dict)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "print(f\"masked pos: {masked_pos}\")\n",
    "print(f\"masked_pos shape: {masked_pos.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "masked_pos = masked_pos[:,:,None]\n",
    "print(f\"masked pos after unsqueeze: {masked_pos.shape}\")\n",
    "print(f\"masked pos: {masked_pos}\")\n",
    "\n",
    "masked_pos = masked_pos.expand(-1,-1,output.size(-1))\n",
    "print(f\"masked pos after expanding: {masked_pos.shape}\")\n",
    "print(f\"masked pos: {masked_pos}\") \n",
    "\n",
    "# final shape of masked_pos : (batch_size,max_pred,d_model)\n",
    "\n",
    "# masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before: torch.Size([2, 3, 5, 7])\n",
      "shape after : torch.Size([2, 3, 1, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------test ---------------------------------------------------------\n",
    "##--------------------------example------------------------------- ##\n",
    "# https://stackoverflow.com/questions/69797614/indexing-a-tensor-with-none-in-pytorch\n",
    "test = torch.rand((2,3,5,7))\n",
    "\n",
    "print(f\"shape before: {test.shape}\")\n",
    "test =test[:,:,None,:,:]\n",
    "\n",
    "print(f\"shape after : {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "test = torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\n",
    "# tensor([[ 1,  1],\n",
    "#         [ 4,  3]])\n",
    "\n",
    "# [\n",
    "#     [ 1 , 2 ]\n",
    "#     [ 3 , 4]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor(\n",
    "    [\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "    ]\n",
    "    \n",
    "    )\n",
    "index = torch.tensor([(0,0),(0,1),(1,0),(1,1)])\n",
    "for (i,j) in index:\n",
    "    print(t[i,j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[i][j] = input[index[i][j]][j]  # if dim == 0\n",
    "# out[i][j] = input[i][index[i][j]]  # if dim == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "# out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "# out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gathering of dim = 0 we need of index (i)\n",
    "# for gathering of dim = 1 we need index of (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor(2) tensor(3) tensor(4)\n",
      "tensor([1, 2]) tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# axis = 1\n",
    "print(\n",
    "t[0][0],\n",
    "t[0][1],\n",
    "t[1][0],\n",
    "t[1][1],\n",
    ")\n",
    "# axis = 0\n",
    "print(\n",
    "    t[0],\n",
    "    t[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\n",
    "    [\n",
    "        [0,1,2],\n",
    "        [3,4,5],\n",
    "        [6,7,8],\n",
    "    ],\n",
    "    [\n",
    "        [9,10,11],\n",
    "        [12,13,14],\n",
    "        [15,16,17],\n",
    "    ],\n",
    "    [\n",
    "        [18,19,20],\n",
    "        [21,22,23],\n",
    "        [24,25,26],\n",
    "    ]\n",
    "]\n",
    "test = torch.tensor(test)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]) tensor([[ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]]) tensor([[18, 19, 20],\n",
      "        [21, 22, 23],\n",
      "        [24, 25, 26]])\n",
      "\n",
      "\n",
      "tensor([0, 1, 2]) tensor([3, 4, 5]) tensor([6, 7, 8])\n"
     ]
    }
   ],
   "source": [
    "# dim 0\n",
    "print(\n",
    "    test[0],\n",
    "    test[1],\n",
    "    test[2]\n",
    ")\n",
    "\n",
    "# for accessing the item in dim=0 you need 2 (i,j) more indices\n",
    "#----------------------------------------------------\n",
    "# dim 1\n",
    "print(f\"\\n\")\n",
    "print(\n",
    "    test[0,0],\n",
    "    test[0,1],\n",
    "    test[0,2],\n",
    ")\n",
    "\n",
    "# for dim = 1 you need  1 (i) indices more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 29\n",
      "embedding weights shape : torch.Size([30, 768])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(30,768)\n",
    "\n",
    "embedding_weights = embedding.weight\n",
    "print(f\"vocab size : {vocab_size}\")\n",
    "print(f\"embedding weights shape : {embedding_weights.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "610c9d3786a5bcd122c38aa183fd980592c5db79f4cee84c2a544f8f000107e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
